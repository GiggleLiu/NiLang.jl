<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Bessel function · NiLang.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">NiLang.jl</span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../why/">What and Why</a></li><li><span class="tocitem">Tutorial</span><ul><li><a class="tocitem" href="../../tutorial/">My first NiLang program</a></li><li><a class="tocitem" href="../port_zygote/">How to port NiLang to Zygote</a></li></ul></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../fib/">Computing Fibonacci Numbers</a></li><li class="is-active"><a class="tocitem" href>Bessel function</a><ul class="internal"><li><a class="tocitem" href="#CUDA-programming-1"><span>CUDA programming</span></a></li><li><a class="tocitem" href="#Benchmark-1"><span>Benchmark</span></a></li></ul></li><li><a class="tocitem" href="../sparse/">Sparse matrices</a></li><li><a class="tocitem" href="../lognumber/">Logarithmic number system</a></li><li><a class="tocitem" href="../unitary/">Unitary matrix operations without allocation</a></li><li><a class="tocitem" href="../qr/">A simple QR decomposition</a></li><li><a class="tocitem" href="../nice/">NICE network</a></li><li><a class="tocitem" href="../realnvp/">RealNVP network</a></li><li><a class="tocitem" href="../boxmuller/">Box-Muller method to Generate normal distribution</a></li></ul></li><li><span class="tocitem">API &amp; Manual</span><ul><li><a class="tocitem" href="../../instructions/">Instruction Reference</a></li><li><a class="tocitem" href="../../extend/">How to extend</a></li><li><a class="tocitem" href="../sharedwrite/">The shared write problem on GPU</a></li><li><a class="tocitem" href="../../api/">API Manual</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Examples</a></li><li class="is-active"><a href>Bessel function</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Bessel function</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/GiggleLiu/NiLang.jl/blob/master/examples/besselj.jl" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Bessel-function-1"><a class="docs-heading-anchor" href="#Bessel-function-1">Bessel function</a><a class="docs-heading-anchor-permalink" href="#Bessel-function-1" title="Permalink"></a></h1><p>An Bessel function of the first kind of order <span>$\nu$</span> can be computed using Taylor expansion</p><div>\[    J_\nu(z) = \sum\limits_{n=0}^{\infty} \frac{(z/2)^\nu}{\Gamma(k+1)\Gamma(k+\nu+1)} (-z^2/4)^{n}\]</div><p>where <span>$\Gamma(n) = (n-1)!$</span> is the Gamma function. One can compute the accumulated item iteratively as <span>$s_n = -\frac{z^2}{4} s_{n-1}$</span>. Intuitively, this problem mimics the famous pebble game, since one can not release state <span>$s_{n-1}$</span> directly after computing <span>$s_n$</span>. One would need an increasing size of tape to cache the intermediate state. To circumvent this problem. We introduce the following reversible approximate multiplier</p><pre><code class="language-julia">using NiLang, NiLang.AD</code></pre><p>Here, the definition of SWAP can be found in \App{app:instr}, <span>$anc! \approx 0$</span> is a <em>dirty ancilla</em>. Line 2 computes the result and accumulates it to the dirty ancilla, we get an approximately correct output in <strong>anc!</strong>. Line 3 &quot;uncomputes&quot; <strong>out!</strong> approximately by using the information stored in <strong>anc!</strong>, leaving a dirty zero state in register <strong>out!</strong>. Line 4 swaps the contents in <strong>out!</strong> and <strong>anc!</strong>. Finally, we have an approximately correct output and a dirtier ancilla. With this multiplier, we implementation <span>$J_\nu$</span> as follows.</p><pre><code class="language-julia">@i function ibesselj(out!::T, ν, z::T; atol=1e-8) where T
    @routine @invcheckoff begin
        k ← 0
        fact_nu ← zero(ν)
        @zeros T halfz halfz_power_nu halfz_power_2 out_anc anc1 anc2 anc3 anc4 anc5
        halfz += z / 2
        halfz_power_nu += halfz ^ ν
        halfz_power_2 += halfz ^ 2
        i_factorial(fact_nu, ν)
        anc1 += halfz_power_nu/fact_nu
        out_anc += anc1
        while (abs(unwrap(anc1)) &gt; atol &amp;&amp; abs(unwrap(anc4)) &lt; atol, k!=0)
            INC(k)
            @routine begin
                anc5 += k + ν
                anc2 -= k * anc5
                anc3 += halfz_power_2 / anc2
            end
            i_dirtymul(anc1, anc3, anc4)
            out_anc += anc1
            ~@routine
        end
    end
    out! += out_anc
    ~@routine
end</code></pre><p>where the <strong>i_factorial</strong> is defined as</p><p>Here, only a constant number of ancillas are used in this implementation, while the algorithm complexity does not increase comparing to its irreversible counterpart. ancilla <strong>anc4</strong> plays the role of <em>dirty ancilla</em> in multiplication, it is uncomputed rigoriously in the uncomputing stage. The reason why the &quot;approximate uncomputing&quot; trick works here lies in the fact that from the mathematic perspective the state in <span>$n$</span>th step <span>$\{s_n, z\}$</span> contains the same amount of information as the state in the <span>$n-1$</span>th step <span>$\{s_{n-1}, z\}$</span> except some special points, it is highly possible to find an equation to uncompute the previous state from the current state. This trick can be used extensively in many other application. It mitigated the artifitial irreversibility brought by the number system that we have adopt at the cost of precision.</p><p>To obtain gradients, one call <strong>Grad(ibesselj)</strong></p><pre><code class="language-julia">y, x = 0.0, 1.0
Grad(ibesselj)(Val(1), y, 2, x)</code></pre><pre><code class="language-none">(Val{1}(), GVar(0.0, 1.0), 2, GVar(1.0, 0.2102436158593427))</code></pre><p>Here, <strong>Grad(ibesselj)</strong> is a callable instance of type <strong>Grad{typeof(ibesselj)}}</strong>. The first parameter <code>Val(1)</code> indicates the first argument is the loss.</p><p>To obtain second order gradients, one can Feed dual numbers to this gradient function.</p><pre><code class="language-julia">using ForwardDiff: Dual
_, hxy, _, hxx = Grad(ibesselj)(Val(1), Dual(y, zero(y)), 2, Dual(x, one(x)))
println(&quot;The hessian dy^2/dx^2 is $(grad(hxx).partials[1])&quot;)</code></pre><pre><code class="language-none">The hessian dy^2/dx^2 is 0.13446683844358093</code></pre><p>Here, the gradient field is a Dual number, it has a field partials that stores the derivative with respect to <code>x</code>. This is the Hessian that we need.</p><h2 id="CUDA-programming-1"><a class="docs-heading-anchor" href="#CUDA-programming-1">CUDA programming</a><a class="docs-heading-anchor-permalink" href="#CUDA-programming-1" title="Permalink"></a></h2><p>The AD in NiLang avoids most heap allocation, so that it is able to execute on a GPU device We suggest using <a href="https://github.com/JuliaGPU/KernelAbstractions.jl">KernelAbstraction</a>, it provides compatibility between CPU and GPU. To execute the above function on GPU, we need only 11 lines of code.</p><pre><code class="language-julia">using CuArrays, GPUArrays, KernelAbstractions

@i @kernel function bessel_kernel(out!, v, z)
    @invcheckoff i ← @index(Global)
    ibesselj(out![i], v, z[i])
    @invcheckoff i → @index(Global)
end</code></pre><p>We have a macro support to KernelAbstraction in NiLang. So it is possible to launch directly like.</p><pre><code class="language-julia">@i function befunc(out!, v::Integer, z)
    @launchkernel CUDA() 256 length(out!) bessel_kernel(out!, v, z)
end</code></pre><p>It is equivalent to call</p><pre><code class="language-julia">(~bessel_kernel)(CUDA(), 256)(out!, v, z; ndrange=length(out!))</code></pre><p>But it will execute the job eagerly for you. We will consider better support in the future.</p><p>Except it is reversible</p><pre><code class="language-julia">julia&gt; @code_reverse @launchkernel CUDA() 256 length(out!) bessel_kernel(out!, v, z)
:(#= REPL[4]:1 =# @launchkernel CUDA() 256 length(out!) (~bessel_kernel)(out!, v, z))</code></pre><p>To test this function, we first define input parameters <code>a</code> and output <code>out!</code></p><pre><code class="language-julia">a = CuArray(rand(128))
out! = CuArray(zeros(128))</code></pre><p>We wrap the output with a randomly initialized gradient field, suppose we get the gradients from a virtual loss function. Also, we need to initialize an empty gradient field for elements in input cuda tensor <code>a</code>.</p><pre><code class="language-julia">out! = ibesselj(out!, 2, GVar.(a))[1]
out_g! = GVar.(out!, CuArray(randn(128)))</code></pre><p>Call the inverse program, the multiple dispatch will drive you to the goal.</p><pre><code class="language-julia">(~ibesselj)(out_g!, 2, GVar.(a))</code></pre><p>You will get CUDA arrays with <code>GVar</code> elements as output, their gradient fields are what you want. Cheers! Now you have a adjoint mode differentiable CUDA kernel.</p><h2 id="Benchmark-1"><a class="docs-heading-anchor" href="#Benchmark-1">Benchmark</a><a class="docs-heading-anchor-permalink" href="#Benchmark-1" title="Permalink"></a></h2><p>We have different source to souce automatic differention implementations of the first type Bessel function <span>$J_2(1.0)$</span> benchmarked and show the results below.</p><table><tr><th style="text-align: right">Package</th><th style="text-align: right">Tangent/Adjoint</th><th style="text-align: right"><span>$T_{\rm min}$</span>/ns</th><th style="text-align: right">Space/KB</th></tr><tr><td style="text-align: right">Julia</td><td style="text-align: right">-</td><td style="text-align: right">22</td><td style="text-align: right">0</td></tr><tr><td style="text-align: right">NiLang</td><td style="text-align: right">-</td><td style="text-align: right">59</td><td style="text-align: right">0</td></tr><tr><td style="text-align: right">ForwardDiff</td><td style="text-align: right">Tangent</td><td style="text-align: right">35</td><td style="text-align: right">0</td></tr><tr><td style="text-align: right">Manual</td><td style="text-align: right">Adjoint</td><td style="text-align: right">83</td><td style="text-align: right">0</td></tr><tr><td style="text-align: right">NiLang.AD</td><td style="text-align: right">Adjoint</td><td style="text-align: right">213</td><td style="text-align: right">0</td></tr><tr><td style="text-align: right">NiLang.AD (GPU)</td><td style="text-align: right">Adjoint</td><td style="text-align: right">1.4</td><td style="text-align: right">0</td></tr><tr><td style="text-align: right">Zygote</td><td style="text-align: right">Adjoint</td><td style="text-align: right">31201</td><td style="text-align: right">13.47</td></tr><tr><td style="text-align: right">Tapenade</td><td style="text-align: right">Adjoint</td><td style="text-align: right">?</td><td style="text-align: right">?</td></tr></table><p>Julia is the CPU time used for running the irreversible forward program, is the baseline of benchmarking. NiLang is the reversible implementation, it is 2.7 times slower than its irreversible counterpart. Here, we have remove the reversibility check. ForwardDiff gives the best performance because it is designed for functions with single input. It is even faster than manually derived gradients</p><div>\[\frac{\partial J_{\nu}(z)}{\partial z} = \frac{J_{\nu-1} - J_{\nu+1}}{2}\]</div><p>NiLang.AD is the reversible differential programming implementation, it considers only the backward pass. The benchmark of its GPU version is estimated on Nvidia Titan V by broadcasting the gradient function on CUDA array of size <span>$2^17$</span> and take average. The Zygote benchmark considers both forward pass and backward pass. Tapenade is not yet ready.</p><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../fib/">« Computing Fibonacci Numbers</a><a class="docs-footer-nextpage" href="../sparse/">Sparse matrices »</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Saturday 8 August 2020 04:21">Saturday 8 August 2020</span>. Using Julia version 1.4.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
