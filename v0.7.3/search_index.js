var documenterSearchIndex = {"docs":
[{"location":"examples/fib/#","page":"Computing Fibonacci Numbers","title":"Computing Fibonacci Numbers","text":"EditURL = \"https://github.com/GiggleLiu/NiLang.jl/blob/master/examples/fib.jl\"","category":"page"},{"location":"examples/fib/#Computing-Fibonacci-Numbers-1","page":"Computing Fibonacci Numbers","title":"Computing Fibonacci Numbers","text":"","category":"section"},{"location":"examples/fib/#","page":"Computing Fibonacci Numbers","title":"Computing Fibonacci Numbers","text":"The following is an example that everyone likes, computing Fibonacci number recursively.","category":"page"},{"location":"examples/fib/#","page":"Computing Fibonacci Numbers","title":"Computing Fibonacci Numbers","text":"using NiLang\n\n@i function rfib(out!, n::T) where T\n    n1 ← zero(T)\n    n2 ← zero(T)\n    @routine begin\n        n1 += n - 1\n        n2 += n - 2\n    end\n    if (value(n) <= 2, ~)\n        out! += 1\n    else\n        rfib(out!, n1)\n        rfib(out!, n2)\n    end\n    ~@routine\nend","category":"page"},{"location":"examples/fib/#","page":"Computing Fibonacci Numbers","title":"Computing Fibonacci Numbers","text":"The time complexity of this recursive algorithm is exponential to input n. It is also possible to write a reversible linear time with for loops. A slightly non-trivial task is computing the first Fibonacci number that greater or equal to a certain number z, where a while statement is required.","category":"page"},{"location":"examples/fib/#","page":"Computing Fibonacci Numbers","title":"Computing Fibonacci Numbers","text":"@i function rfibn(n!, z)\n    @safe @assert n! == 0\n    out ← 0\n    rfib(out, n!)\n    while (out < z, n! != 0)\n        ~rfib(out, n!)\n        n! += 1\n        rfib(out, n!)\n    end\n    ~rfib(out, n!)\nend","category":"page"},{"location":"examples/fib/#","page":"Computing Fibonacci Numbers","title":"Computing Fibonacci Numbers","text":"In this example, the postcondition n!=0 in the while statement is false before entering the loop, and it becomes true in later iterations. In the reverse program, the while statement stops at n==0. If executed correctly, a user will see the following result.","category":"page"},{"location":"examples/fib/#","page":"Computing Fibonacci Numbers","title":"Computing Fibonacci Numbers","text":"rfib(0, 10)","category":"page"},{"location":"examples/fib/#","page":"Computing Fibonacci Numbers","title":"Computing Fibonacci Numbers","text":"compute which the first Fibonacci number greater than 100.","category":"page"},{"location":"examples/fib/#","page":"Computing Fibonacci Numbers","title":"Computing Fibonacci Numbers","text":"rfibn(0, 100)","category":"page"},{"location":"examples/fib/#","page":"Computing Fibonacci Numbers","title":"Computing Fibonacci Numbers","text":"and uncompute","category":"page"},{"location":"examples/fib/#","page":"Computing Fibonacci Numbers","title":"Computing Fibonacci Numbers","text":"(~rfibn)(rfibn(0, 100)...)","category":"page"},{"location":"examples/fib/#","page":"Computing Fibonacci Numbers","title":"Computing Fibonacci Numbers","text":"This example shows how an addition postcondition provided by the user can help to reverse a control flow without caching controls.","category":"page"},{"location":"examples/fib/#","page":"Computing Fibonacci Numbers","title":"Computing Fibonacci Numbers","text":"","category":"page"},{"location":"examples/fib/#","page":"Computing Fibonacci Numbers","title":"Computing Fibonacci Numbers","text":"This page was generated using Literate.jl.","category":"page"},{"location":"examples/unitary/#","page":"Unitary matrix operations without allocation","title":"Unitary matrix operations without allocation","text":"EditURL = \"https://github.com/GiggleLiu/NiLang.jl/blob/master/examples/unitary.jl\"","category":"page"},{"location":"examples/unitary/#Unitary-matrix-operations-without-allocation-1","page":"Unitary matrix operations without allocation","title":"Unitary matrix operations without allocation","text":"","category":"section"},{"location":"examples/unitary/#","page":"Unitary matrix operations without allocation","title":"Unitary matrix operations without allocation","text":"A unitary matrix features uniform eigenvalues and reversibility. It is widely used as an approach to ease the gradient exploding and vanishing problem and the memory wall problem. One of the simplest ways to parametrize a unitary matrix is representing a unitary matrix as a product of two-level unitary operations. A real unitary matrix of size N can be parametrized compactly by N(N-1)2 rotation operations","category":"page"},{"location":"examples/unitary/#","page":"Unitary matrix operations without allocation","title":"Unitary matrix operations without allocation","text":"   rm ROT(a b theta)  = left(beginmatrix\n       cos(theta)  - sin(theta)\n       sin(theta)   cos(theta)\n   endmatrixright)\n   left(beginmatrix\n       a\n       b\n   endmatrixright)","category":"page"},{"location":"examples/unitary/#","page":"Unitary matrix operations without allocation","title":"Unitary matrix operations without allocation","text":"where theta is the rotation angle, a! and b! are target registers.","category":"page"},{"location":"examples/unitary/#","page":"Unitary matrix operations without allocation","title":"Unitary matrix operations without allocation","text":"using NiLang, NiLang.AD\n\n@i function umm!(x!, θ)\n    @safe @assert length(θ) ==\n            length(x!)*(length(x!)-1)/2\n    k ← 0\n    for j=1:length(x!)\n        for i=length(x!)-1:-1:j\n            k += 1\n            ROT(x![i], x![i+1], θ[k])\n        end\n    end\n\n    k → length(θ)\nend","category":"page"},{"location":"examples/unitary/#","page":"Unitary matrix operations without allocation","title":"Unitary matrix operations without allocation","text":"Here, the ancilla k is deallocated manually by specifying its value, because we know the loop size is N(N-1)2. We define the test functions in order to check gradients.","category":"page"},{"location":"examples/unitary/#","page":"Unitary matrix operations without allocation","title":"Unitary matrix operations without allocation","text":"@i function isum(out!, x::AbstractArray)\n    for i=1:length(x)\n        out! += x[i]\n    end\nend\n\n@i function test!(out!, x!::Vector, θ::Vector)\n   umm!(x!, θ)\n   isum(out!, x!)\nend","category":"page"},{"location":"examples/unitary/#","page":"Unitary matrix operations without allocation","title":"Unitary matrix operations without allocation","text":"Let's print the program output","category":"page"},{"location":"examples/unitary/#","page":"Unitary matrix operations without allocation","title":"Unitary matrix operations without allocation","text":"out, x, θ = 0.0, randn(4), randn(6);\n@instr Grad(test!)(Val(1), out, x, θ)\nx","category":"page"},{"location":"examples/unitary/#","page":"Unitary matrix operations without allocation","title":"Unitary matrix operations without allocation","text":"We can erease the gradient field by uncomputing the gradient function. If you want, you can differentiate it twice to obtain Hessians. However, we suggest using ForwardDifferentiation over our NiLang program, this is more efficient.","category":"page"},{"location":"examples/unitary/#","page":"Unitary matrix operations without allocation","title":"Unitary matrix operations without allocation","text":"@instr (~Grad(test!))(Val(1), out, x, θ)\nx","category":"page"},{"location":"examples/unitary/#","page":"Unitary matrix operations without allocation","title":"Unitary matrix operations without allocation","text":"In the above testing code, Grad(test) attaches a gradient field to each element of x. ~Grad(test) is the inverse program that erase the gradient fields. Notably, this reversible implementation costs zero memory allocation, although it changes the target variables inplace.","category":"page"},{"location":"examples/unitary/#","page":"Unitary matrix operations without allocation","title":"Unitary matrix operations without allocation","text":"","category":"page"},{"location":"examples/unitary/#","page":"Unitary matrix operations without allocation","title":"Unitary matrix operations without allocation","text":"This page was generated using Literate.jl.","category":"page"},{"location":"examples/sparse/#","page":"Sparse matrices","title":"Sparse matrices","text":"EditURL = \"https://github.com/GiggleLiu/NiLang.jl/blob/master/examples/sparse.jl\"","category":"page"},{"location":"examples/sparse/#Sparse-matrices-1","page":"Sparse matrices","title":"Sparse matrices","text":"","category":"section"},{"location":"examples/sparse/#","page":"Sparse matrices","title":"Sparse matrices","text":"Source to source automatic differentiation is useful in differentiating sparse matrices. It is a well-known problem that sparse matrix operations can not benefit directly from generic backward rules for dense matrices because general rules do not keep the sparse structure. In the following, we will show that reversible AD can differentiate the Frobenius dot product between two sparse matrices with the state-of-the-art performance. Here, the Frobenius dot product is defined as \\texttt{trace(A'B)}. Its native Julia (irreversible) implementation is SparseArrays.dot.","category":"page"},{"location":"examples/sparse/#","page":"Sparse matrices","title":"Sparse matrices","text":"The following is a reversible counterpart","category":"page"},{"location":"examples/sparse/#","page":"Sparse matrices","title":"Sparse matrices","text":"using NiLang, NiLang.AD\nusing SparseArrays\n\n@i function idot(r::T, A::SparseMatrixCSC{T},B::SparseMatrixCSC{T}) where {T}\n    m ← size(A, 1)\n    n ← size(A, 2)\n    @invcheckoff branch_keeper ← zeros(Bool, 2*m)\n    @safe size(B) == (m,n) || throw(DimensionMismatch(\"matrices must have the same dimensions\"))\n    @invcheckoff @inbounds for j = 1:n\n        ia1 ← A.colptr[j]\n        ib1 ← B.colptr[j]\n        ia2 ← A.colptr[j+1]\n        ib2 ← B.colptr[j+1]\n        ia ← ia1\n        ib ← ib1\n        @inbounds for i=1:ia2-ia1+ib2-ib1-1\n            ra ← A.rowval[ia]\n            rb ← B.rowval[ib]\n            if (ra == rb, ~)\n                r += A.nzval[ia]' * B.nzval[ib]\n            end\n            # b move -> true, a move -> false\n            branch_keeper[i] ⊻= ia == ia2-1 || ra > rb\n            ra → A.rowval[ia]\n            rb → B.rowval[ib]\n            if (branch_keeper[i], ~)\n                INC(ib)\n            else\n                INC(ia)\n            end\n        end\n        ~@inbounds for i=1:ia2-ia1+ib2-ib1-1\n            # b move -> true, a move -> false\n            branch_keeper[i] ⊻= ia == ia2-1 || A.rowval[ia] > B.rowval[ib]\n            if (branch_keeper[i], ~)\n                INC(ib)\n            else\n                INC(ia)\n            end\n        end\n    end\n    @invcheckoff branch_keeper → zeros(Bool, 2*m)\nend","category":"page"},{"location":"examples/sparse/#","page":"Sparse matrices","title":"Sparse matrices","text":"Here, the key point is using a \\texttt{branch_keeper} vector to cache branch decisions.","category":"page"},{"location":"examples/sparse/#","page":"Sparse matrices","title":"Sparse matrices","text":"The time used for a native implementation is","category":"page"},{"location":"examples/sparse/#","page":"Sparse matrices","title":"Sparse matrices","text":"using BenchmarkTools\na = sprand(1000, 1000, 0.01);\nb = sprand(1000, 1000, 0.01);\n@benchmark SparseArrays.dot($a, $b)","category":"page"},{"location":"examples/sparse/#","page":"Sparse matrices","title":"Sparse matrices","text":"To compute the gradients, we wrap each matrix element with GVar, and send them to the reversible backward pass","category":"page"},{"location":"examples/sparse/#","page":"Sparse matrices","title":"Sparse matrices","text":"out! = SparseArrays.dot(a, b)\n@benchmark (~idot)($(GVar(out!, 1.0)),\n        $(GVar.(a)), $(GVar.(b)))","category":"page"},{"location":"examples/sparse/#","page":"Sparse matrices","title":"Sparse matrices","text":"The time used for computing backward pass is approximately 1.6 times Julia's native forward pass. Here, we have turned off the reversibility check off to achieve better performance. By writing sparse matrix multiplication and other sparse matrix operations reversibly, we will have a differentiable sparse matrix library with proper performance.","category":"page"},{"location":"examples/sparse/#","page":"Sparse matrices","title":"Sparse matrices","text":"See my another blog post for reversible sparse matrix multiplication.","category":"page"},{"location":"examples/sparse/#","page":"Sparse matrices","title":"Sparse matrices","text":"","category":"page"},{"location":"examples/sparse/#","page":"Sparse matrices","title":"Sparse matrices","text":"This page was generated using Literate.jl.","category":"page"},{"location":"extend/#How-to-extend-1","page":"How to extend","title":"How to extend","text":"","category":"section"},{"location":"extend/#Extend-,-and-for-irreversible-one-out-functions-1","page":"How to extend","title":"Extend +=, -= and ⊻= for irreversible one-out functions","text":"","category":"section"},{"location":"extend/#","page":"How to extend","title":"How to extend","text":"It directly works","category":"page"},{"location":"extend/#","page":"How to extend","title":"How to extend","text":"julia> using SpecialFunctions, NiLang\n\njulia> x, y = 2.1, 1.0\n(2.1, 1.0)\n\njulia> @instr y += besselj0(x)\n2.1\n\njulia> x, y\n(2.1, 1.7492472503018073)\n\njulia> @instr ~(y += besselj0(x))\n2.1\n\njulia> x, y\n(2.1, 1.0)","category":"page"},{"location":"extend/#","page":"How to extend","title":"How to extend","text":"Here the statement","category":"page"},{"location":"extend/#","page":"How to extend","title":"How to extend","text":"@instr y += besselj0(x)","category":"page"},{"location":"extend/#","page":"How to extend","title":"How to extend","text":"is mapped to","category":"page"},{"location":"extend/#","page":"How to extend","title":"How to extend","text":"@instr ⊕(besselj0)(y, x)","category":"page"},{"location":"extend/#","page":"How to extend","title":"How to extend","text":"However, doing this does not give you correct gradients. For y += scalar_out_function(x), one can bind the backward rules like","category":"page"},{"location":"extend/#","page":"How to extend","title":"How to extend","text":"julia> using ChainRules, NiLang.AD\n\njulia> besselj0_back(x) = ChainRules.rrule(besselj0, x)[2](1.0)[2]\nbesselj0_back (generic function with 1 method)\n\njulia> primitive_grad(::typeof(besselj0), x::Real) = besselj0_back(x)\nprimitive_grad (generic function with 1 method)\n\njulia> xg, yg = GVar(x), GVar(y, 1.0)\n(GVar(2.1, 0.0), GVar(1.0, 1.0))\n\njulia> @instr yg -= besselj0(xg)\nGVar(2.1, -0.5682921357570385)\n\njulia> xg, yg\n(GVar(2.1, -0.5682921357570385), GVar(0.8333930196680097, 1.0))\n\njulia> @instr yg += besselj0(xg)\nGVar(2.1, 0.0)\n\njulia> xg, yg\n(GVar(2.1, 0.0), GVar(1.0, 1.0))\n\njulia> NiLang.AD.check_grad(⊕(besselj0), (1.0, 2.1); iloss=1)\ntrue\n\njulia> using BenchmarkTools\n\njulia> @benchmark ⊕(besselj0)($yg, $xg)\nBenchmarkTools.Trial: \n  memory estimate:  0 bytes\n  allocs estimate:  0\n  --------------\n  minimum time:     451.523 ns (0.00% GC)\n  median time:      459.431 ns (0.00% GC)\n  mean time:        477.419 ns (0.00% GC)\n  maximum time:     857.036 ns (0.00% GC)\n  --------------\n  samples:          10000\n  evals/sample:     197","category":"page"},{"location":"extend/#","page":"How to extend","title":"How to extend","text":"Good!","category":"page"},{"location":"extend/#Reversible-multi-in-multi-out-functions-1","page":"How to extend","title":"Reversible multi-in multi-out functions","text":"","category":"section"},{"location":"extend/#","page":"How to extend","title":"How to extend","text":"It is easy to do, define two normal Julia functions reversible to each other, using the macro @dual to tell the compiler they are reversible to each other.","category":"page"},{"location":"extend/#","page":"How to extend","title":"How to extend","text":"For example, a pair of dual functions ROT (2D rotation) and IROT (inverse rotation) that already defined in NiLang.","category":"page"},{"location":"extend/#","page":"How to extend","title":"How to extend","text":"\"\"\"\n    ROT(a!, b!, θ) -> a!', b!', θ\n\"\"\"\n@inline function ROT(i::Real, j::Real, θ::Real)\n    a, b = rot(i, j, θ)\n    a, b, θ\nend\n\n\"\"\"\n    IROT(a!, b!, θ) -> ROT(a!, b!, -θ)\n\"\"\"\n@inline function IROT(i::Real, j::Real, θ::Real)\n    i, j, _ = ROT(i, j, -θ)\n    i, j, θ\nend\n@dual ROT IROT","category":"page"},{"location":"extend/#","page":"How to extend","title":"How to extend","text":"One can easily check the reversibility by typing","category":"page"},{"location":"extend/#","page":"How to extend","title":"How to extend","text":"julia> check_inv(ROT, (1.0, 2.0, 3.0))\ntrue","category":"page"},{"location":"extend/#","page":"How to extend","title":"How to extend","text":"For self-reversible functions, one can declare the reversibility for it like this","category":"page"},{"location":"extend/#","page":"How to extend","title":"How to extend","text":"\"\"\"\n    SWAP(a!, b!) -> b!, a!\n\"\"\"\n@inline function SWAP(a!::Real, b!::Real)\n    b!, a!\nend\n@selfdual SWAP","category":"page"},{"location":"extend/#","page":"How to extend","title":"How to extend","text":"To bind gradients for this multi-in, multi-out function. The general approach is Binding the backward rule on its inverse!","category":"page"},{"location":"extend/#","page":"How to extend","title":"How to extend","text":"@i @inline function IROT(a!::GVar, b!::GVar, θ::GVar)\n    IROT(value(a!), value(b!), value(θ))\n    -(value(θ))\n    value(θ) -= π/2\n    ROT(grad(a!), grad(b!), value(θ))\n    grad(θ) += value(a!) * grad(a!)\n    grad(θ) += value(b!) * grad(b!)\n    value(θ) += π/2\n    -(value(θ))\n    ROT(grad(a!), grad(b!), π/2)\nend\n\n@i @inline function IROT(a!::GVar, b!::GVar, θ::Real)\n    IROT(value(a!), value(b!), θ)\n    -(θ)\n    θ -= π/2\n    ROT(grad(a!), grad(b!), θ)\n    θ += π/2\n    -(θ)\n    ROT(grad(a!), grad(b!), π/2)\nend\n\n@nograd IROT(a!::Real, b!::Real, θ::GVar)","category":"page"},{"location":"extend/#","page":"How to extend","title":"How to extend","text":"When this inverse function is called, the backward rules are automatically applied.","category":"page"},{"location":"extend/#","page":"How to extend","title":"How to extend","text":"Good! This method can also be extended to linear algebra functions, however, the memory allocation overhead is high because one need to wrap each element with GVar.","category":"page"},{"location":"examples/lognumber/#","page":"Logarithmic number system","title":"Logarithmic number system","text":"EditURL = \"https://github.com/GiggleLiu/NiLang.jl/blob/master/examples/lognumber.jl\"","category":"page"},{"location":"examples/lognumber/#Logarithmic-number-system-1","page":"Logarithmic number system","title":"Logarithmic number system","text":"","category":"section"},{"location":"examples/lognumber/#","page":"Logarithmic number system","title":"Logarithmic number system","text":"Computing basic functions like power, exp and besselj is not trivial for reversible programming. There is no efficient constant memory algorithm using pure fixed point numbers only. For example, to compute x ^ n reversiblly with fixed point numbers, we need to allocate a vector of size O(n). With logarithmic numbers, the above computation is straight forward.","category":"page"},{"location":"examples/lognumber/#","page":"Logarithmic number system","title":"Logarithmic number system","text":"using LogarithmicNumbers\nusing NiLang, NiLang.AD\nusing FixedPointNumbers\n\n@i function i_power(y::T, x::T, n::Int) where T\n    @routine begin\n        lx ← one(ULogarithmic{T})\n        ly ← one(ULogarithmic{T})\n        # convert `x` to a logarithmic number\n        # Here, `*=` is reversible for log numbers\n        lx *= convert(x)\n        for i=1:n\n            ly *= lx\n        end\n    end\n\n    # convert back to fixed point numbers\n    y += convert(ly)\n\n    ~@routine\nend","category":"page"},{"location":"examples/lognumber/#","page":"Logarithmic number system","title":"Logarithmic number system","text":"To check the function","category":"page"},{"location":"examples/lognumber/#","page":"Logarithmic number system","title":"Logarithmic number system","text":"i_power(Fixed43(0.0), Fixed43(0.4), 3)","category":"page"},{"location":"examples/lognumber/#exp-function-as-an-example-1","page":"Logarithmic number system","title":"exp function as an example","text":"","category":"section"},{"location":"examples/lognumber/#","page":"Logarithmic number system","title":"Logarithmic number system","text":"The following example computes exp(x).","category":"page"},{"location":"examples/lognumber/#","page":"Logarithmic number system","title":"Logarithmic number system","text":"@i function i_exp(y!::T, x::T) where T<:Union{Fixed, GVar{<:Fixed}}\n    @invcheckoff begin\n        @routine begin\n            s ← one(ULogarithmic{T})\n            lx ← one(ULogarithmic{T})\n            k ← 0\n        end\n        lx *= convert(x)\n        y! += convert(s)\n        while (s.log > -20, k != 0)\n            k += 1\n            s *= lx / k\n            y! += convert(s)\n        end\n        ~(while (s.log > -20, k != 0)\n            k += 1\n            s *= x / k\n        end)\n        lx /= convert(x)\n        ~@routine\n    end\nend\n\nx = Fixed43(3.5)","category":"page"},{"location":"examples/lognumber/#","page":"Logarithmic number system","title":"Logarithmic number system","text":"We can check the reversibility","category":"page"},{"location":"examples/lognumber/#","page":"Logarithmic number system","title":"Logarithmic number system","text":"out, _ = i_exp(Fixed43(0.0), x)\n@assert out ≈ exp(3.5)","category":"page"},{"location":"examples/lognumber/#","page":"Logarithmic number system","title":"Logarithmic number system","text":"Computing the gradients","category":"page"},{"location":"examples/lognumber/#","page":"Logarithmic number system","title":"Logarithmic number system","text":"_, gx = NiLang.AD.gradient(Val(1), i_exp, (Fixed43(0.0), x))\n@assert gx ≈ exp(3.5)","category":"page"},{"location":"examples/lognumber/#","page":"Logarithmic number system","title":"Logarithmic number system","text":"","category":"page"},{"location":"examples/lognumber/#","page":"Logarithmic number system","title":"Logarithmic number system","text":"This page was generated using Literate.jl.","category":"page"},{"location":"examples/boxmuller/#","page":"Box-Muller method to Generate normal distribution","title":"Box-Muller method to Generate normal distribution","text":"EditURL = \"https://github.com/GiggleLiu/NiLang.jl/blob/master/examples/boxmuller.jl\"","category":"page"},{"location":"examples/boxmuller/#Box-Muller-method-to-Generate-normal-distribution-1","page":"Box-Muller method to Generate normal distribution","title":"Box-Muller method to Generate normal distribution","text":"","category":"section"},{"location":"examples/boxmuller/#","page":"Box-Muller method to Generate normal distribution","title":"Box-Muller method to Generate normal distribution","text":"using NiLang","category":"page"},{"location":"examples/boxmuller/#","page":"Box-Muller method to Generate normal distribution","title":"Box-Muller method to Generate normal distribution","text":"In this tutorial, we introduce using Box-Muller method to transform a uniform distribution to a normal distribution. The transformation and inverse transformation of Box-Muller method could be found in this blog","category":"page"},{"location":"examples/boxmuller/#","page":"Box-Muller method to Generate normal distribution","title":"Box-Muller method to Generate normal distribution","text":"@i function boxmuller(x::T, y::T) where T\n    @routine @invcheckoff begin\n        @zeros T θ logx _2logx\n        θ += 2π * y\n        logx += log(x)\n        _2logx += -2 * logx\n    end\n\n    # store results\n    z1 ← zero(T)\n    z2 ← zero(T)\n    z1 += _2logx ^ 0.5\n    ROT(z1, z2, θ)\n    ~@routine\n\n    SWAP(x, z1)\n    SWAP(y, z2)\n\n    # arithmetic uncomputing: recomputing the original values of `x` and `y` to deallocate z1 and z2\n    @routine @invcheckoff begin\n        @zeros T at sq _halfsq\n        at += atan(y, x)\n        if (y < 0, ~)\n            at += T(2π)\n        end\n        sq += x ^ 2\n        sq += y ^ 2\n        _halfsq -= sq / 2\n    end\n    z1 -= exp(_halfsq)\n    z2 -= at / (2π)\n    @invcheckoff z1 → zero(T)\n    @invcheckoff z2 → zero(T)\n    ~@routine\nend","category":"page"},{"location":"examples/boxmuller/#","page":"Box-Muller method to Generate normal distribution","title":"Box-Muller method to Generate normal distribution","text":"One may wonder why this implementation is so long, should't NiLang generate the inverse for user? The fact is, although Box-Muller is arithmetically reversible. It is not finite precision reversible. Hence we need to \"uncompute\" it manually, this trick may introduce reversibility error.","category":"page"},{"location":"examples/boxmuller/#","page":"Box-Muller method to Generate normal distribution","title":"Box-Muller method to Generate normal distribution","text":"using Plots\nN = 5000\nx = rand(2*N)\n\nPlots.histogram(x, bins = -3:0.1:3, label=\"uniform\",\n    legendfontsize=16, xtickfontsize=16, ytickfontsize=16)","category":"page"},{"location":"examples/boxmuller/#","page":"Box-Muller method to Generate normal distribution","title":"Box-Muller method to Generate normal distribution","text":"forward","category":"page"},{"location":"examples/boxmuller/#","page":"Box-Muller method to Generate normal distribution","title":"Box-Muller method to Generate normal distribution","text":"@instr boxmuller.(x[1:N], x[N+1:end])\nPlots.histogram(x, bins = -3:0.1:3, label=\"normal\",\n    legendfontsize=16, xtickfontsize=16, ytickfontsize=16)","category":"page"},{"location":"examples/boxmuller/#","page":"Box-Muller method to Generate normal distribution","title":"Box-Muller method to Generate normal distribution","text":"backward","category":"page"},{"location":"examples/boxmuller/#","page":"Box-Muller method to Generate normal distribution","title":"Box-Muller method to Generate normal distribution","text":"@instr (~boxmuller).(x[1:N], x[N+1:end])\nPlots.histogram(x, bins = -3:0.1:3, label=\"uniform\",\n    legendfontsize=16, xtickfontsize=16, ytickfontsize=16)","category":"page"},{"location":"examples/boxmuller/#Check-the-probability-distribution-function-1","page":"Box-Muller method to Generate normal distribution","title":"Check the probability distribution function","text":"","category":"section"},{"location":"examples/boxmuller/#","page":"Box-Muller method to Generate normal distribution","title":"Box-Muller method to Generate normal distribution","text":"using LinearAlgebra, Test\n\nnormalpdf(x) = sqrt(1/2π)*exp(-x^2/2)","category":"page"},{"location":"examples/boxmuller/#","page":"Box-Muller method to Generate normal distribution","title":"Box-Muller method to Generate normal distribution","text":"obtain log(abs(det(jacobians)))","category":"page"},{"location":"examples/boxmuller/#","page":"Box-Muller method to Generate normal distribution","title":"Box-Muller method to Generate normal distribution","text":"@i function f(x::Vector)\n    boxmuller(x[1], x[2])\nend\njac = NiLang.AD.jacobian(f, [0.5, 0.5], iin=1)\nladj = log(abs(det(jac)))","category":"page"},{"location":"examples/boxmuller/#","page":"Box-Muller method to Generate normal distribution","title":"Box-Muller method to Generate normal distribution","text":"check if it matches the log(p/q).","category":"page"},{"location":"examples/boxmuller/#","page":"Box-Muller method to Generate normal distribution","title":"Box-Muller method to Generate normal distribution","text":"z1, z2 = boxmuller(0.5, 0.5)\n@test ladj ≈ log(1.0 / (normalpdf(z1) * normalpdf(z2)))","category":"page"},{"location":"examples/boxmuller/#To-obtaining-Jacobian-a-simpler-approach-1","page":"Box-Muller method to Generate normal distribution","title":"To obtaining Jacobian - a simpler approach","text":"","category":"section"},{"location":"examples/boxmuller/#","page":"Box-Muller method to Generate normal distribution","title":"Box-Muller method to Generate normal distribution","text":"We can define a function that exactly reversible from the instruction level, but costs more space for storing output.","category":"page"},{"location":"examples/boxmuller/#","page":"Box-Muller method to Generate normal distribution","title":"Box-Muller method to Generate normal distribution","text":"@i function boxmuller2(x1::T, x2::T, z1::T, z2::T) where T\n    @routine @invcheckoff begin\n        @zeros T θ logx _2logx\n\n        θ += 2π * x2\n        logx += log(x1)\n        _2logx += -2 * logx\n    end\n\n    # store results\n    z1 += _2logx ^ 0.5\n    ROT(z1, z2, θ)\n    ~@routine\nend","category":"page"},{"location":"examples/boxmuller/#","page":"Box-Muller method to Generate normal distribution","title":"Box-Muller method to Generate normal distribution","text":"However, this is not a bijector from that maps x to z, because computing the backward just erases the content in z. However, this function can be used to obtain log(abs(det(jacobians)))","category":"page"},{"location":"examples/boxmuller/#","page":"Box-Muller method to Generate normal distribution","title":"Box-Muller method to Generate normal distribution","text":"@i function f2(x::Vector, z::Vector)\n    boxmuller2(x[1], x[2], z[1], z[2])\nend\njac = NiLang.AD.jacobian(f2, [0.5, 0.5], [0.0, 0.0], iin=1, iout=2)\nladj = log(abs(det(jac)))","category":"page"},{"location":"examples/boxmuller/#","page":"Box-Muller method to Generate normal distribution","title":"Box-Muller method to Generate normal distribution","text":"check if it matches the log(p/q).","category":"page"},{"location":"examples/boxmuller/#","page":"Box-Muller method to Generate normal distribution","title":"Box-Muller method to Generate normal distribution","text":"_, _, z1, z2 = boxmuller2(0.5, 0.5, 0.0, 0.0)\n@test ladj ≈ log(1.0 / (normalpdf(z1) * normalpdf(z2)))","category":"page"},{"location":"examples/boxmuller/#","page":"Box-Muller method to Generate normal distribution","title":"Box-Muller method to Generate normal distribution","text":"","category":"page"},{"location":"examples/boxmuller/#","page":"Box-Muller method to Generate normal distribution","title":"Box-Muller method to Generate normal distribution","text":"This page was generated using Literate.jl.","category":"page"},{"location":"examples/qr/#","page":"A simple QR decomposition","title":"A simple QR decomposition","text":"EditURL = \"https://github.com/GiggleLiu/NiLang.jl/blob/master/examples/qr.jl\"","category":"page"},{"location":"examples/qr/#A-simple-QR-decomposition-1","page":"A simple QR decomposition","title":"A simple QR decomposition","text":"","category":"section"},{"location":"examples/qr/#Functions-used-in-this-example-1","page":"A simple QR decomposition","title":"Functions used in this example","text":"","category":"section"},{"location":"examples/qr/#","page":"A simple QR decomposition","title":"A simple QR decomposition","text":"using NiLang, NiLang.AD","category":"page"},{"location":"examples/qr/#The-QR-decomposition-1","page":"A simple QR decomposition","title":"The QR decomposition","text":"","category":"section"},{"location":"examples/qr/#","page":"A simple QR decomposition","title":"A simple QR decomposition","text":"Let us consider a naive implementation of QR decomposition from scratch. This implementation is just a proof of principle which does not consider reorthogonalization and other practical issues.","category":"page"},{"location":"examples/qr/#","page":"A simple QR decomposition","title":"A simple QR decomposition","text":"@i function qr(Q, R, A::Matrix{T}) where T\n    anc_norm ← zero(T)\n    anc_dot ← zeros(T, size(A,2))\n    ri ← zeros(T, size(A,1))\n    for col = 1:size(A, 1)\n        ri .+= A[:,col]\n        for precol = 1:col-1\n            i_dot(anc_dot[precol], Q[:,precol], ri)\n            R[precol,col] += anc_dot[precol]\n            for row = 1:size(Q,1)\n                ri[row] -=\n                    anc_dot[precol] * Q[row, precol]\n            end\n        end\n        i_norm2(anc_norm, ri)\n\n        R[col, col] += anc_norm^0.5\n        for row = 1:size(Q,1)\n            Q[row,col] += ri[row] / R[col, col]\n        end\n\n        ~begin\n            ri .+= A[:,col]\n            for precol = 1:col-1\n                i_dot(anc_dot[precol], Q[:,precol], ri)\n                for row = 1:size(Q,1)\n                    ri[row] -= anc_dot[precol] *\n                        Q[row, precol]\n                end\n            end\n            i_norm2(anc_norm, ri)\n        end\n    end\nend","category":"page"},{"location":"examples/qr/#","page":"A simple QR decomposition","title":"A simple QR decomposition","text":"Here, in order to avoid frequent uncomputing, we allocate ancillas ri and anc_dot as vectors. The expression in ~ is used to uncompute ri, anc_dot and anc_norm. i_dot and i_norm2 are reversible functions to compute dot product and vector norm. One can quickly check the correctness of the gradient function","category":"page"},{"location":"examples/qr/#","page":"A simple QR decomposition","title":"A simple QR decomposition","text":"A  = randn(4,4)\nq, r = zero(A), zero(A)\n@i function test1(out, q, r, A)\n    qr(q, r, A)\n    i_sum(out, q)\nend\n\ncheck_grad(test1, (0.0, q, r, A); iloss=1)","category":"page"},{"location":"examples/qr/#","page":"A simple QR decomposition","title":"A simple QR decomposition","text":"Here, the loss function test1 is defined as the sum of the output unitary matrix q. The check_grad function is a gradient checker function defined in module NiLang.AD.","category":"page"},{"location":"examples/qr/#","page":"A simple QR decomposition","title":"A simple QR decomposition","text":"","category":"page"},{"location":"examples/qr/#","page":"A simple QR decomposition","title":"A simple QR decomposition","text":"This page was generated using Literate.jl.","category":"page"},{"location":"api/#","page":"API Manual","title":"API Manual","text":"DocTestSetup = quote\n    using NiLangCore, NiLang, NiLang.AD, Test\nend","category":"page"},{"location":"api/#API-Manual-1","page":"API Manual","title":"API Manual","text":"","category":"section"},{"location":"api/#Compiling-Tools-(Reexported-from-NiLangCore)-1","page":"API Manual","title":"Compiling Tools (Reexported from NiLangCore)","text":"","category":"section"},{"location":"api/#","page":"API Manual","title":"API Manual","text":"Modules = [NiLangCore]\nOrder   = [:macro, :function, :type]","category":"page"},{"location":"api/#NiLangCore.@assign","page":"API Manual","title":"NiLangCore.@assign","text":"@assign a b [invcheck]\n\nPerform the assign a = b in a reversible program. Turn off invertibility check if the invcheck is false.\n\n\n\n\n\n","category":"macro"},{"location":"api/#NiLangCore.@assignback","page":"API Manual","title":"NiLangCore.@assignback","text":"@assignback f(args...) [invcheck]\n\nAssign input variables with output values: args... = f(args...), turn off invertibility error check if the second argument is false.\n\n\n\n\n\n","category":"macro"},{"location":"api/#NiLangCore.@code_julia-Tuple{Any}","page":"API Manual","title":"NiLangCore.@code_julia","text":"@code_julia ex\n\nGet the interpreted expression of ex.\n\njulia> using MacroTools\n\njulia> prettify(@code_julia x += exp(3.0))\n:(@assignback (PlusEq(exp))(x, 3.0))\n\n\n\n\n\n","category":"macro"},{"location":"api/#NiLangCore.@code_preprocess-Tuple{Any}","page":"API Manual","title":"NiLangCore.@code_preprocess","text":"@code_preprocess ex\n\nPreprocess ex and return the symmetric reversible IR.\n\njulia> using MacroTools\n\njulia> prettify(@code_preprocess if (x < 3, ~) x += exp(3.0) end)\n:(if (x < 3, x < 3)\n      x += exp(3.0)\n  else\n  end)\n\n\n\n\n\n","category":"macro"},{"location":"api/#NiLangCore.@code_reverse-Tuple{Any}","page":"API Manual","title":"NiLangCore.@code_reverse","text":"@code_reverse ex\n\nGet the reversed expression of ex.\n\njulia> @code_reverse x += exp(3.0)\n:(x -= exp(3.0))\n\n\n\n\n\n","category":"macro"},{"location":"api/#NiLangCore.@dual-Tuple{Any,Any}","page":"API Manual","title":"NiLangCore.@dual","text":"@dual f invf\n\nDefine f and invf as a pair of dual instructions, i.e. reverse to each other.\n\n\n\n\n\n","category":"macro"},{"location":"api/#NiLangCore.@fieldview-Tuple{Any}","page":"API Manual","title":"NiLangCore.@fieldview","text":"@fieldview fname(x::TYPE) = x.fieldname\n\nCreate a function fieldview that can be accessed by a reversible program\n\njulia> struct GVar{T, GT}\n           x::T\n           g::GT\n       end\n\njulia> @fieldview xx(x::GVar) = x.x\n\njulia> chfield(GVar(1.0, 0.0), xx, 2.0)\nGVar{Float64,Float64}(2.0, 0.0)\n\n\n\n\n\n","category":"macro"},{"location":"api/#NiLangCore.@i-Tuple{Any}","page":"API Manual","title":"NiLangCore.@i","text":"@i function fname(args..., kwargs...) ... end\n@i struct sname ... end\n\nDefine a reversible function/type.\n\njulia> @i function test(out!, x)\n           out! += identity(x)\n       end\n\njulia> test(0.2, 0.8)\n(1.0, 0.8)\n\njulia> @i struct CVar{T}\n           g::T\n           x::T\n           function CVar{T}(x::T, g::T) where T\n               new{T}(x, g)\n           end\n           function CVar(x::T, g::T) where T\n               new{T}(x, g)\n           end\n           # warning: infered type `T` should be used in `← new` statement only.\n           @i function CVar(xx::T) where T\n               gg ← zero(xx)\n               gg += identity(1)\n               xx ← new{T}(gg, xx)\n           end\n       end\n\njulia> CVar(0.2)\nCVar{Float64}(1.0, 0.2)\n\njulia> (~CVar)(CVar(0.2))\n0.2\n\nSee test/compiler.jl and test/invtype.jl for more examples.\n\n\n\n\n\n","category":"macro"},{"location":"api/#NiLangCore.@icast-Tuple{Any,Any}","page":"API Manual","title":"NiLangCore.@icast","text":"@icast TDEF1 => TDEF2 begin ... end\n\nDefine type cast between TDEF1 and TDEF2. Type TDEF1 or TDEF2 can be T(args...) or x::T like.\n\njulia> struct PVar{T}\n           g::T\n           x::T\n       end\n\njulia> struct SVar{T}\n           x::T\n           g::T\n       end\n\njulia> @icast PVar(g, x) => SVar(x, k) begin\n          g → zero(x)\n          k ← zero(x)\n          k += identity(x)\n       end\n\njulia> x = PVar(0.0, 0.5)\nPVar{Float64}(0.0, 0.5)\n\njulia> @instr (PVar=>SVar)(x)\nSVar{Float64}(0.5, 0.5)\n\njulia> @instr (SVar=>PVar)(x)\nPVar{Float64}(0.0, 0.5)\n\njulia> @icast x::Base.Float64 => SVar(x, gg) begin\n           gg ← zero(x)\n           gg += identity(x)\n       end\n\njulia> x = 0.5\n0.5\n\njulia> @instr (Float64=>SVar)(x)\nSVar{Float64}(0.5, 0.5)\n\njulia> @instr (SVar=>Float64)(x)\n0.5\n\n\n\n\n\n","category":"macro"},{"location":"api/#NiLangCore.@instr-Tuple{Any}","page":"API Manual","title":"NiLangCore.@instr","text":"@instr ex\n\nExecute a reversible instruction.\n\n\n\n\n\n","category":"macro"},{"location":"api/#NiLangCore.@invcheck-Tuple{Any}","page":"API Manual","title":"NiLangCore.@invcheck","text":"@invcheck ex\n@invcheck x val\n\nPass the check it if ex is true or x ≈ val.\n\n\n\n\n\n","category":"macro"},{"location":"api/#NiLangCore.@pure_wrapper-Tuple{Any}","page":"API Manual","title":"NiLangCore.@pure_wrapper","text":"@pure_wrapper TYPE\n\nCreate a reversible wrapper type TYPE{T} <: IWrapper{T} that plays a role of simple wrapper.\n\njulia> @pure_wrapper A\n\njulia> A(0.5)\nA(0.5)\n\njulia> (~A)(A(0.5))\n0.5\n\njulia> -A(0.5)\nA(-0.5)\n\njulia> A(0.5) < A(0.6)\ntrue\n\n\n\n\n\n","category":"macro"},{"location":"api/#NiLangCore.@selfdual-Tuple{Any}","page":"API Manual","title":"NiLangCore.@selfdual","text":"@selfdual f\n\nDefine f as a self-dual instructions.\n\n\n\n\n\n","category":"macro"},{"location":"api/#NiLangCore.almost_same-Union{Tuple{T}, Tuple{T,T}} where T<:Number","page":"API Manual","title":"NiLangCore.almost_same","text":"almost_same(a, b; atol=GLOBAL_ATOL[], kwargs...) -> Bool\n\nReturn true if a and b are almost same w.r.t. atol.\n\n\n\n\n\n","category":"method"},{"location":"api/#NiLangCore.check_inv-Tuple{Any,Any}","page":"API Manual","title":"NiLangCore.check_inv","text":"check_inv(f, args; atol::Real=1e-8, verbose::Bool=false, kwargs...)\n\nReturn true if f(args..., kwargs...) is reversible.\n\n\n\n\n\n","category":"method"},{"location":"api/#NiLangCore.chfield","page":"API Manual","title":"NiLangCore.chfield","text":"chfield(x, field, val)\n\nChange a field of an object x.\n\nThe field can be a Val type\n\njulia> chfield(1+2im, Val(:im), 5)\n1 + 5im\n\nor a function\n\njulia> using NiLangCore\n\njulia> struct GVar{T, GT}\n           x::T\n           g::GT\n       end\n\njulia> @fieldview xx(x::GVar) = x.x\n\njulia> chfield(GVar(1.0, 0.0), xx, 2.0)\nGVar{Float64,Float64}(2.0, 0.0)\n\n\n\n\n\n","category":"function"},{"location":"api/#NiLangCore.isprimitive-Tuple{Any}","page":"API Manual","title":"NiLangCore.isprimitive","text":"isprimitive(f)\n\nReturn true if f is an instruction that can not be decomposed anymore.\n\n\n\n\n\n","category":"method"},{"location":"api/#NiLangCore.isreflexive-Tuple{Any}","page":"API Manual","title":"NiLangCore.isreflexive","text":"isreflexive(f)\n\nReturn true if a function is self-inverse.\n\n\n\n\n\n","category":"method"},{"location":"api/#NiLangCore.isreversible-Union{Tuple{ARGT}, Tuple{Any,Type{ARGT}}} where ARGT","page":"API Manual","title":"NiLangCore.isreversible","text":"isreversible(f, ARGT)\n\nReturn true if a function is reversible.\n\n\n\n\n\n","category":"method"},{"location":"api/#NiLangCore.nilang_ir-Tuple{Module,Any}","page":"API Manual","title":"NiLangCore.nilang_ir","text":"nilang_ir(ex; reversed::Bool=false)\n\nGet the NiLang reversible IR from the function expression ex, return the reversed function if reversed is true.\n\nThis IR is not directly executable on Julia, please use macroexpand(Main, :(@i function .... end)) to get the julia expression of a reversible function.\n\njulia> ex = :(@inline function f(x!::T, y) where T\n               anc ← zero(T)\n               @routine anc += identity(x!)\n               x! += y * anc\n               ~@routine\n           end);\n\njulia> nilang_ir(ex) |> MacroTools.prettify\n:(@inline function f(x!::T, y) where T\n          anc ← zero(T)\n          anc += identity(x!)\n          x! += y * anc\n          anc -= identity(x!)\n          anc → zero(T)\n      end)\n\njulia> nilang_ir(ex; reversed=true) |> MacroTools.prettify\n:(@inline function (~f)(x!::T, y) where T\n          anc ← zero(T)\n          anc += identity(x!)\n          x! -= y * anc\n          anc -= identity(x!)\n          anc → zero(T)\n      end)\n\n\n\n\n\n","category":"method"},{"location":"api/#NiLangCore.tget-Tuple{Int64}","page":"API Manual","title":"NiLangCore.tget","text":"tget(i::Int)\n\nGet the i-th entry of a tuple.\n\n\n\n\n\n","category":"method"},{"location":"api/#NiLangCore.unwrap-Tuple{IWrapper}","page":"API Manual","title":"NiLangCore.unwrap","text":"unwrap(x)\n\nUnwrap a wrapper instance (recursively) to get the original value.\n\n\n\n\n\n","category":"method"},{"location":"api/#NiLangCore.value-Tuple{Any}","page":"API Manual","title":"NiLangCore.value","text":"value(x)\n\nGet the value from a wrapper instance.\n\n\n\n\n\n","category":"method"},{"location":"api/#NiLangCore.DivEq","page":"API Manual","title":"NiLangCore.DivEq","text":"DivEq{FT} <: Function\nDivEq(f)\n\nCalled when executing out /= f(args...) instruction. See PlusEq for detail.\n\n\n\n\n\n","category":"type"},{"location":"api/#NiLangCore.IWrapper","page":"API Manual","title":"NiLangCore.IWrapper","text":"IWrapper{T} <: Real\n\nIWrapper{T} is a wrapper of for data of type T. It will forward >, <, >=, <=, ≈ operations.\n\n\n\n\n\n","category":"type"},{"location":"api/#NiLangCore.Inv","page":"API Manual","title":"NiLangCore.Inv","text":"Inv{FT} <: Function\nInv(f)\n\nThe inverse of a function.\n\n\n\n\n\n","category":"type"},{"location":"api/#NiLangCore.InvertibilityError","page":"API Manual","title":"NiLangCore.InvertibilityError","text":"InvertibilityError <: Exception\nInvertibilityError(ex)\n\nThe error thrown when a irreversible statement appears in a reversible context.\n\n\n\n\n\n","category":"type"},{"location":"api/#NiLangCore.MinusEq","page":"API Manual","title":"NiLangCore.MinusEq","text":"MinusEq{FT} <: Function\nMinusEq(f)\n⊖(f)\n\nCalled when executing out -= f(args...) instruction. See PlusEq for detail.\n\n\n\n\n\n","category":"type"},{"location":"api/#NiLangCore.MulEq","page":"API Manual","title":"NiLangCore.MulEq","text":"MulEq{FT} <: Function\nMulEq(f)\n\nCalled when executing out *= f(args...) instruction. See PlusEq for detail.\n\n\n\n\n\n","category":"type"},{"location":"api/#NiLangCore.Partial","page":"API Manual","title":"NiLangCore.Partial","text":"Partial{FIELD, T, T2} <: IWrapper{T2}\n\nTake a field FIELD without dropping information. This operation can be undone by calling ~Partial{FIELD}.\n\n\n\n\n\n","category":"type"},{"location":"api/#NiLangCore.PlusEq","page":"API Manual","title":"NiLangCore.PlusEq","text":"PlusEq{FT} <: Function\nPlusEq(f)\n⊕(f)\n\nCalled when executing out += f(args...) instruction. The following two statements are same\n\njulia> x, y, z = 0.0, 2.0, 3.0\n(0.0, 2.0, 3.0)\n\njulia> x, y, z = PlusEq(*)(x, y, z)\n(6.0, 2.0, 3.0)\n\njulia> x, y, z = 0.0, 2.0, 3.0\n(0.0, 2.0, 3.0)\n\njulia> @instr x += y*z\n3.0\n\njulia> x, y, z\n(6.0, 2.0, 3.0)\n\n\n\n\n\n","category":"type"},{"location":"api/#NiLangCore.XorEq","page":"API Manual","title":"NiLangCore.XorEq","text":"XorEq{FT} <: Function\nXorEq(f)\n⊙(f)\n\nCalled when executing out ⊻= f(args...) instruction. See PlusEq for detail.\n\n\n\n\n\n","category":"type"},{"location":"api/#NiLangCore.assign_vars-Tuple{Any,Any}","page":"API Manual","title":"NiLangCore.assign_vars","text":"assign_vars(args, symres; invcheck)\n\nGet the expression of assigning symres to args.\n\n\n\n\n\n","category":"method"},{"location":"api/#NiLangCore.bcast_assign_vars-Tuple{Any,Any}","page":"API Manual","title":"NiLangCore.bcast_assign_vars","text":"The broadcast version of assign_vars\n\n\n\n\n\n","category":"method"},{"location":"api/#NiLangCore.compile_ex-Tuple{Module,Any,Any}","page":"API Manual","title":"NiLangCore.compile_ex","text":"translate to normal julia code.\n\n\n\n\n\n","category":"method"},{"location":"api/#NiLangCore.dual_ex-Tuple{Module,Any}","page":"API Manual","title":"NiLangCore.dual_ex","text":"dual_ex(m::Module, ex)\n\nGet the dual expression of ex.\n\n\n\n\n\n","category":"method"},{"location":"api/#NiLangCore.struct2namedtuple-Tuple{Any}","page":"API Manual","title":"NiLangCore.struct2namedtuple","text":"convert an object to a named tuple.\n\n\n\n\n\n","category":"method"},{"location":"api/#Instructions-1","page":"API Manual","title":"Instructions","text":"","category":"section"},{"location":"api/#","page":"API Manual","title":"API Manual","text":"Modules = [NiLang]\nOrder   = [:macro, :function, :type]","category":"page"},{"location":"api/#NiLang.@zeros-Tuple{Any,Vararg{Any,N} where N}","page":"API Manual","title":"NiLang.@zeros","text":"Create zeros of specific type.\n\njulia> @i function f(x)\n           @zeros Float64 a b c\n           # do something\n       end\n\n\n\n\n\n","category":"macro"},{"location":"api/#NiLang.DEC-Tuple{Number}","page":"API Manual","title":"NiLang.DEC","text":"DEC(a!) -> a! - 1\n\n\n\n\n\n","category":"method"},{"location":"api/#NiLang.INC-Tuple{Number}","page":"API Manual","title":"NiLang.INC","text":"INC(a!) -> a! + 1\n\n\n\n\n\n","category":"method"},{"location":"api/#NiLang.IROT-Tuple{Real,Real,Real}","page":"API Manual","title":"NiLang.IROT","text":"IROT(a!, b!, θ) -> ROT(a!, b!, -θ)\n\n\n\n\n\n","category":"method"},{"location":"api/#NiLang.ROT-Tuple{Real,Real,Real}","page":"API Manual","title":"NiLang.ROT","text":"ROT(a!, b!, θ) -> a!', b!', θ\n\nbeginalign\n    rm ROT(a b theta)  = beginbmatrix\n        cos(theta)  - sin(theta)\n        sin(theta)   cos(theta)\n    endbmatrix\n    beginbmatrix\n        a\n        b\n    endbmatrix\nendalign\n\n\n\n\n\n","category":"method"},{"location":"api/#NiLang.SWAP-Union{Tuple{T}, Tuple{T,T}} where T","page":"API Manual","title":"NiLang.SWAP","text":"SWAP(a!, b!) -> b!, a!\n\n\n\n\n\n","category":"method"},{"location":"api/#NiLang.arshift-Union{Tuple{T}, Tuple{T,Any}} where T","page":"API Manual","title":"NiLang.arshift","text":"arshift(x, n)\n\nright shift, sign extending.\n\n\n\n\n\n","category":"method"},{"location":"api/#NiLang.i_affine!-Union{Tuple{T}, Tuple{AbstractArray{T,1},AbstractArray{T,2},AbstractArray{T,1},AbstractArray{T,1}}} where T","page":"API Manual","title":"NiLang.i_affine!","text":"i_affine!(y!, W, b, x)\n\naffine! transformation y! += W*x + b.\n\n\n\n\n\n","category":"method"},{"location":"api/#NiLang.i_ascending!-Union{Tuple{T}, Tuple{AbstractArray{T,1},Any,AbstractArray{T,N} where N}} where T","page":"API Manual","title":"NiLang.i_ascending!","text":"i_ascending!(xs!, inds!, arr)\n\nFind the ascending sequence in arr and store the results into xs!, indices are stored in inds!. This function can be used to get the maximum value and maximum indices.\n\n\n\n\n\n","category":"method"},{"location":"api/#NiLang.i_axpy!-Tuple{Any,Any,Any}","page":"API Manual","title":"NiLang.i_axpy!","text":"i_axpy!(a, x, y!)\n\ncompute y! += a * x, where x and y are vectors.\n\n\n\n\n\n","category":"method"},{"location":"api/#NiLang.i_dirtymul-Tuple{Any,Any,Any}","page":"API Manual","title":"NiLang.i_dirtymul","text":"i_dirtymul(out!, x, anc!)\n\n\"dirty\" reversible multiplication that computes out! *= x approximately for floating point numbers, the anc! is anticipated as a number ~0.\n\n\n\n\n\n","category":"method"},{"location":"api/#NiLang.i_factorial-Tuple{Int64,Int64}","page":"API Manual","title":"NiLang.i_factorial","text":"i_factorial(out!, n)\n\nCompute the factorial out! = factorial(n).\n\n\n\n\n\n","category":"method"},{"location":"api/#NiLang.i_filter!-Union{Tuple{T}, Tuple{Any,AbstractArray{T,1} where T,AbstractArray{T,1}}} where T","page":"API Manual","title":"NiLang.i_filter!","text":"i_filter!(f, out!, iter)\n\nReversible filter function, out! is an emptied vector.\n\n\n\n\n\n","category":"method"},{"location":"api/#NiLang.i_inv!-Union{Tuple{T}, Tuple{AbstractArray{T,2},AbstractArray{T,2}}} where T","page":"API Manual","title":"NiLang.i_inv!","text":"i_inv!(out!, A)\n\nGet the inverse of A.\n\nthis function is implemented as a primitive.\n\n\n\n\n\n","category":"method"},{"location":"api/#NiLang.i_logsumexp-Union{Tuple{T}, Tuple{Any,Any,Any,Any,AbstractArray{T,N} where N}} where T","page":"API Manual","title":"NiLang.i_logsumexp","text":"i_logsumexp(logout!, out!, xs!, inds!, x)\n\nCompute logout! = log(sum(exp(x))).\n\nArguments\n\n* `out!`, output,\n* `logout!`, logged output,\n* `xs!`, an empty vector to cache the ascending values (same type as `x`),\n* `inds!`, an empty vector to cache the ascending indices (integer type),\n* `x`, input vector.\n\n\n\n\n\n","category":"method"},{"location":"api/#NiLang.i_mapfoldl-Union{Tuple{T}, Tuple{Any,Any,T,Any}} where T","page":"API Manual","title":"NiLang.i_mapfoldl","text":"i_mapfoldl(map, fold, out!, iter)\n\nReversible mapfoldl function, map can be irreversible, but fold should be reversible.\n\n\n\n\n\n","category":"method"},{"location":"api/#NiLang.i_mean_sum-Tuple{Any,Any,Any}","page":"API Manual","title":"NiLang.i_mean_sum","text":"i_mean_sum(out!, sum!, x)\n\nget the mean and sum of x.\n\n\n\n\n\n","category":"method"},{"location":"api/#NiLang.i_mul!-Union{Tuple{T}, Tuple{AbstractArray{T,2},AbstractArray{T,2},AbstractArray{T,2}}} where T","page":"API Manual","title":"NiLang.i_mul!","text":"i_mul!(out!, x, y)\n\ncompute x * y (x and y are matrices, and store results in out!.\n\n\n\n\n\n","category":"method"},{"location":"api/#NiLang.i_norm2-Tuple{Any,Any}","page":"API Manual","title":"NiLang.i_norm2","text":"i_norm2(out!, x)\n\nget the squared norm of x.\n\n\n\n\n\n","category":"method"},{"location":"api/#NiLang.i_normal_logpdf-Union{Tuple{T}, Tuple{Any,T,Any,Any}} where T","page":"API Manual","title":"NiLang.i_normal_logpdf","text":"i_normal_logpdf(out, x, μ, σ)\n\nget the pdf of Normal(μ, σ) at point x.\n\n\n\n\n\n","category":"method"},{"location":"api/#NiLang.i_relu-Tuple{Any,Any}","page":"API Manual","title":"NiLang.i_relu","text":"i_relu(out!, x)\n\nReLU in machine learning.\n\n\n\n\n\n","category":"method"},{"location":"api/#NiLang.i_softmax_crossentropy-Union{Tuple{T}, Tuple{Any,Any,Any,Any,Any,T}} where T","page":"API Manual","title":"NiLang.i_softmax_crossentropy","text":"i_softmax_crossentropy(x, p, imax, xmax, Z, out)\n\nSoftmax-Cross entropy function.\n\n\n\n\n\n","category":"method"},{"location":"api/#NiLang.i_sqdistance-Union{Tuple{T}, Tuple{Any,AbstractArray{T,1},AbstractArray{T,1} where T}} where T","page":"API Manual","title":"NiLang.i_sqdistance","text":"i_sqdistance(dist!, x1, x2)\n\nSquared distance between two points x1 and x2.\n\n\n\n\n\n","category":"method"},{"location":"api/#NiLang.i_sum-Tuple{Any,AbstractArray}","page":"API Manual","title":"NiLang.i_sum","text":"i_sum(out!, x)\n\nget the sum of x.\n\n\n\n\n\n","category":"method"},{"location":"api/#NiLang.i_umm!-Tuple{AbstractArray,Any}","page":"API Manual","title":"NiLang.i_umm!","text":"i_umm!(x!, θ)\n\nCompute unitary matrix multiplication on x, where the unitary matrix is parameterized by (N+1)*N/2 θs.\n\n\n\n\n\n","category":"method"},{"location":"api/#NiLang.i_var_mean_sum-Union{Tuple{T}, Tuple{Any,Any,Any,Any,AbstractArray{T,1}}} where T","page":"API Manual","title":"NiLang.i_var_mean_sum","text":"i_var_mean_sum(var!, varsum!, mean!, sum!, sqv)\n\nCompute the variance, the accumulated variance, mean and sum.\n\n\n\n\n\n","category":"method"},{"location":"api/#NiLang.plshift-Tuple{Any,Any}","page":"API Manual","title":"NiLang.plshift","text":"plshift(x, n)\n\nperiodic left shift.\n\n\n\n\n\n","category":"method"},{"location":"api/#NiLang.prshift-Tuple{Any,Any}","page":"API Manual","title":"NiLang.prshift","text":"plshift(x, n)\n\nperiodic right shift.\n\n\n\n\n\n","category":"method"},{"location":"api/#NiLang.rot-Tuple{Any,Any,Any}","page":"API Manual","title":"NiLang.rot","text":"rot(a, b, θ)\n\nrotate variables a and b by an angle θ\n\n\n\n\n\n","category":"method"},{"location":"api/#NiLang.AutoBcast","page":"API Manual","title":"NiLang.AutoBcast","text":"AutoBcast{T,N} <: IWrapper{T}\n\nA vectorized variable.\n\n\n\n\n\n","category":"type"},{"location":"api/#NiLang.NoGrad","page":"API Manual","title":"NiLang.NoGrad","text":"NoGrad{T} <: IWrapper{T}\nNoGrad(x)\n\nA NoGrad(x) is equivalent to GVar^{-1}(x), which cancels the GVar wrapper.\n\n\n\n\n\n","category":"type"},{"location":"api/#Automatic-Differentiation-1","page":"API Manual","title":"Automatic Differentiation","text":"","category":"section"},{"location":"api/#","page":"API Manual","title":"API Manual","text":"Modules = [NiLang.AD]\nOrder   = [:macro, :function, :type]","category":"page"},{"location":"api/#NiLang.AD.@nograd-Tuple{Any}","page":"API Manual","title":"NiLang.AD.@nograd","text":"@nograd f(args...)\n\nMark f(args...) as having no gradients.\n\n\n\n\n\n","category":"macro"},{"location":"api/#NiLang.AD.check_grad-Tuple{Any,Any}","page":"API Manual","title":"NiLang.AD.check_grad","text":"check_grad(f, args; atol::Real=1e-8, verbose::Bool=false, iloss::Int, kwargs...)\n\nReturn true if the gradient of f(args..., kwargs...) is reversible.\n\n\n\n\n\n","category":"method"},{"location":"api/#NiLang.AD.grad-Tuple{NiLang.AD.GVar}","page":"API Manual","title":"NiLang.AD.grad","text":"grad(var)\n\nGet the gradient field of var.\n\n\n\n\n\n","category":"method"},{"location":"api/#NiLang.AD.gradient_numeric-Tuple{Any,Any}","page":"API Manual","title":"NiLang.AD.gradient_numeric","text":"gradient_numeric(f, args...; iloss, kwargs...)\n\nNumeric differentiating f(args..., kwargs...).\n\n\n\n\n\n","category":"method"},{"location":"api/#NiLang.AD.hessian_backback-Tuple{Any,Any}","page":"API Manual","title":"NiLang.AD.hessian_backback","text":"hessian_backback(f, args; iloss::Int, kwargs...)\n\nObtain the Hessian matrix of f(args..., kwargs...) by back propagating adjoint program.\n\n\n\n\n\n","category":"method"},{"location":"api/#NiLang.AD.jacobian-Tuple{Any,Vararg{Any,N} where N}","page":"API Manual","title":"NiLang.AD.jacobian","text":"jacobian(f, args...; iin::Int, iout::Int=iin, kwargs...)\n\nGet the Jacobian matrix for function f(args..., kwargs...) using vectorized variables in the gradient field. One can use key word arguments iin and iout to specify the input and output tensor.\n\n\n\n\n\n","category":"method"},{"location":"api/#NiLang.AD.jacobian_repeat-Tuple{Any,Vararg{Any,N} where N}","page":"API Manual","title":"NiLang.AD.jacobian_repeat","text":"jacobian_repeat(f, args...; iin::Int, iout::Int=iin, kwargs...)\n\nGet the Jacobian matrix for function f(args..., kwargs...) using repeated computing gradients for each output. One can use key word arguments iin and iout to specify the input and output tensor.\n\n\n\n\n\n","category":"method"},{"location":"api/#NiLang.AD.GVar","page":"API Manual","title":"NiLang.AD.GVar","text":"GVar{T,GT} <: IWrapper{T}\nGVar(x)\n\nAttach a gradient field to x.\n\n\n\n\n\n","category":"type"},{"location":"api/#NiLang.AD.NGrad","page":"API Manual","title":"NiLang.AD.NGrad","text":"NGrad{N,FT} <: Function\n\nObtain gradients Grad(f)(Val(i), args..., kwargs...), where i is the index of loss in args. Grad object calls forward first, and then backward.\n\nnote: Note\nVal(1) is specially optimized, so putting the loss as the first parameter can avoid potential overhead.\n\n```\n\n\n\n\n\n","category":"type"},{"location":"examples/sharedwrite/#","page":"The shared write problem on GPU","title":"The shared write problem on GPU","text":"EditURL = \"https://github.com/GiggleLiu/NiLang.jl/blob/master/examples/sharedwrite.jl\"","category":"page"},{"location":"examples/sharedwrite/#The-shared-write-problem-on-GPU-1","page":"The shared write problem on GPU","title":"The shared write problem on GPU","text":"","category":"section"},{"location":"examples/sharedwrite/#","page":"The shared write problem on GPU","title":"The shared write problem on GPU","text":"We will write a GPU version of axpy! function.","category":"page"},{"location":"examples/sharedwrite/#The-main-program-1","page":"The shared write problem on GPU","title":"The main program","text":"","category":"section"},{"location":"examples/sharedwrite/#","page":"The shared write problem on GPU","title":"The shared write problem on GPU","text":"using KernelAbstractions\nusing NiLang, NiLang.AD\nusing CuArrays","category":"page"},{"location":"examples/sharedwrite/#","page":"The shared write problem on GPU","title":"The shared write problem on GPU","text":"so far, this example requires patch: https://github.com/JuliaGPU/KernelAbstractions.jl/pull/52","category":"page"},{"location":"examples/sharedwrite/#","page":"The shared write problem on GPU","title":"The shared write problem on GPU","text":"@i @kernel function axpy_kernel(y!, α, x)\n    # invcheckoff to turn of `reversibility checker`\n    # GPU can not handle errors!\n    @invcheckoff begin\n        i ← @index(Global)\n        y![i] += x[i] * α\n        i → @index(Global)\n    end\nend\n\n@i function cu_axpy!(y!::AbstractVector, α, x::AbstractVector)\n    @launchkernel CUDA() 256 length(y!) axpy_kernel(y!, α, x)\nend\n\n@i function loss(out, y!, α, x)\n    cu_axpy!(y!, α, x)\n    # Note: the following code is stupid scalar operations on CuArray,\n    # They are only for testing.\n    for i=1:length(y!)\n        out += y![i]\n    end\nend\n\ny! = rand(100)\nx = rand(100)\ncuy! = y! |> CuArray\ncux = x |> CuArray\nα = 0.4","category":"page"},{"location":"examples/sharedwrite/#Check-the-correctness-of-results-1","page":"The shared write problem on GPU","title":"Check the correctness of results","text":"","category":"section"},{"location":"examples/sharedwrite/#","page":"The shared write problem on GPU","title":"The shared write problem on GPU","text":"using Test\ncu_axpy!(cuy!, α, cux)\n@test cuy! ≈ y! .+ α .* x\n(~cu_axpy!)(cuy!, α, cux)\n@test cuy! ≈ y!","category":"page"},{"location":"examples/sharedwrite/#","page":"The shared write problem on GPU","title":"The shared write problem on GPU","text":"Let's check the gradients","category":"page"},{"location":"examples/sharedwrite/#","page":"The shared write problem on GPU","title":"The shared write problem on GPU","text":"lsout = 0.0\n@instr Grad(loss)(Val(1), lsout, cuy!, α, cux)","category":"page"},{"location":"examples/sharedwrite/#","page":"The shared write problem on GPU","title":"The shared write problem on GPU","text":"you will see a correct vector [0.4, 0.4, 0.4 ...]","category":"page"},{"location":"examples/sharedwrite/#","page":"The shared write problem on GPU","title":"The shared write problem on GPU","text":"grad.(cux)","category":"page"},{"location":"examples/sharedwrite/#","page":"The shared write problem on GPU","title":"The shared write problem on GPU","text":"you will see 0.0.","category":"page"},{"location":"examples/sharedwrite/#","page":"The shared write problem on GPU","title":"The shared write problem on GPU","text":"grad(α)","category":"page"},{"location":"examples/sharedwrite/#Why-some-gradients-not-correct?-1","page":"The shared write problem on GPU","title":"Why some gradients not correct?","text":"","category":"section"},{"location":"examples/sharedwrite/#","page":"The shared write problem on GPU","title":"The shared write problem on GPU","text":"In the above example, α is a scalar, whereas a scalar is not allowed to change in a CUDA kernel. What if we change α to a CuArray?","category":"page"},{"location":"examples/sharedwrite/#This-one-works:-using-a-vector-of-α-1","page":"The shared write problem on GPU","title":"This one works: using a vector of α","text":"","category":"section"},{"location":"examples/sharedwrite/#","page":"The shared write problem on GPU","title":"The shared write problem on GPU","text":"@i @kernel function axpy_kernel(y!, α, x)\n    @invcheckoff begin\n        i ← @index(Global)\n        y![i] += x[i] * α[i]\n        i → @index(Global)\n    end\nend\n\ncuy! = y! |> CuArray\ncux = x |> CuArray\ncuβ = repeat([0.4], 100) |> CuArray\nlsout = 0.0\n@instr Grad(loss)(Val(1), lsout, cuy!, cuβ, cux)","category":"page"},{"location":"examples/sharedwrite/#","page":"The shared write problem on GPU","title":"The shared write problem on GPU","text":"You will see correct answer","category":"page"},{"location":"examples/sharedwrite/#","page":"The shared write problem on GPU","title":"The shared write problem on GPU","text":"grad.(cuβ)","category":"page"},{"location":"examples/sharedwrite/#This-one-has-the-shared-write-problem:-using-a-vector-of-α,-but-shared-read.-1","page":"The shared write problem on GPU","title":"This one has the shared write problem: using a vector of α, but shared read.","text":"","category":"section"},{"location":"examples/sharedwrite/#","page":"The shared write problem on GPU","title":"The shared write problem on GPU","text":"@i @kernel function axpy_kernel(y!, α, x)\n    @invcheckoff begin\n        i ← @index(Global)\n        y![i] += x[i] * α[i]\n        i → @index(Global)\n    end\nend\n\ncuy! = y! |> CuArray\ncux = x |> CuArray\ncuβ = repeat([0.4], 100) |> CuArray\nlsout = 0.0\ncuβ = [0.4] |> CuArray","category":"page"},{"location":"examples/sharedwrite/#","page":"The shared write problem on GPU","title":"The shared write problem on GPU","text":"Run the following will give you a happy error","category":"page"},{"location":"examples/sharedwrite/#","page":"The shared write problem on GPU","title":"The shared write problem on GPU","text":"ERROR: a exception was thrown during kernel execution.        Run Julia on debug level 2 for device stack traces.","category":"page"},{"location":"examples/sharedwrite/#","page":"The shared write problem on GPU","title":"The shared write problem on GPU","text":"@instr Grad(loss)(Val(1), lsout, cuy!, cuβ, cux)","category":"page"},{"location":"examples/sharedwrite/#","page":"The shared write problem on GPU","title":"The shared write problem on GPU","text":"Because, shared write is not allowed. We need someone clever enough to solve this problem for us.","category":"page"},{"location":"examples/sharedwrite/#Conclusion-1","page":"The shared write problem on GPU","title":"Conclusion","text":"","category":"section"},{"location":"examples/sharedwrite/#","page":"The shared write problem on GPU","title":"The shared write problem on GPU","text":"Shared scalar: the gradient of a scalar will not be updated.\nExpanded vector: works properly, but costs more memory.\nShared 1-element vector: error on shared write.","category":"page"},{"location":"examples/sharedwrite/#","page":"The shared write problem on GPU","title":"The shared write problem on GPU","text":"","category":"page"},{"location":"examples/sharedwrite/#","page":"The shared write problem on GPU","title":"The shared write problem on GPU","text":"This page was generated using Literate.jl.","category":"page"},{"location":"examples/besselj/#","page":"Bessel function","title":"Bessel function","text":"EditURL = \"https://github.com/GiggleLiu/NiLang.jl/blob/master/examples/besselj.jl\"","category":"page"},{"location":"examples/besselj/#Bessel-function-1","page":"Bessel function","title":"Bessel function","text":"","category":"section"},{"location":"examples/besselj/#","page":"Bessel function","title":"Bessel function","text":"An Bessel function of the first kind of order nu can be computed using Taylor expansion","category":"page"},{"location":"examples/besselj/#","page":"Bessel function","title":"Bessel function","text":"    J_nu(z) = sumlimits_n=0^infty frac(z2)^nuGamma(k+1)Gamma(k+nu+1) (-z^24)^n","category":"page"},{"location":"examples/besselj/#","page":"Bessel function","title":"Bessel function","text":"where Gamma(n) = (n-1) is the Gamma function. One can compute the accumulated item iteratively as s_n = -fracz^24 s_n-1. Intuitively, this problem mimics the famous pebble game, since one can not release state s_n-1 directly after computing s_n. One would need an increasing size of tape to cache the intermediate state. To circumvent this problem. We introduce the following reversible approximate multiplier","category":"page"},{"location":"examples/besselj/#","page":"Bessel function","title":"Bessel function","text":"using NiLang, NiLang.AD","category":"page"},{"location":"examples/besselj/#","page":"Bessel function","title":"Bessel function","text":"Here, the definition of SWAP can be found in \\App{app:instr}, anc approx 0 is a dirty ancilla. Line 2 computes the result and accumulates it to the dirty ancilla, we get an approximately correct output in anc!. Line 3 \"uncomputes\" out! approximately by using the information stored in anc!, leaving a dirty zero state in register out!. Line 4 swaps the contents in out! and anc!. Finally, we have an approximately correct output and a dirtier ancilla. With this multiplier, we implementation J_nu as follows.","category":"page"},{"location":"examples/besselj/#","page":"Bessel function","title":"Bessel function","text":"@i function ibesselj(out!::T, ν, z::T; atol=1e-8) where T\n    @routine @invcheckoff begin\n        k ← 0\n        fact_nu ← zero(ν)\n        @zeros T halfz halfz_power_nu halfz_power_2 out_anc anc1 anc2 anc3 anc4 anc5\n        halfz += z / 2\n        halfz_power_nu += halfz ^ ν\n        halfz_power_2 += halfz ^ 2\n        i_factorial(fact_nu, ν)\n        anc1 += halfz_power_nu/fact_nu\n        out_anc += anc1\n        while (abs(unwrap(anc1)) > atol && abs(unwrap(anc4)) < atol, k!=0)\n            INC(k)\n            @routine begin\n                anc5 += k + ν\n                anc2 -= k * anc5\n                anc3 += halfz_power_2 / anc2\n            end\n            i_dirtymul(anc1, anc3, anc4)\n            out_anc += anc1\n            ~@routine\n        end\n    end\n    out! += out_anc\n    ~@routine\nend","category":"page"},{"location":"examples/besselj/#","page":"Bessel function","title":"Bessel function","text":"where the i_factorial is defined as","category":"page"},{"location":"examples/besselj/#","page":"Bessel function","title":"Bessel function","text":"Here, only a constant number of ancillas are used in this implementation, while the algorithm complexity does not increase comparing to its irreversible counterpart. ancilla anc4 plays the role of dirty ancilla in multiplication, it is uncomputed rigoriously in the uncomputing stage. The reason why the \"approximate uncomputing\" trick works here lies in the fact that from the mathematic perspective the state in nth step s_n z contains the same amount of information as the state in the n-1th step s_n-1 z except some special points, it is highly possible to find an equation to uncompute the previous state from the current state. This trick can be used extensively in many other application. It mitigated the artifitial irreversibility brought by the number system that we have adopt at the cost of precision.","category":"page"},{"location":"examples/besselj/#","page":"Bessel function","title":"Bessel function","text":"To obtain gradients, one call Grad(ibesselj)","category":"page"},{"location":"examples/besselj/#","page":"Bessel function","title":"Bessel function","text":"y, x = 0.0, 1.0\nGrad(ibesselj)(Val(1), y, 2, x)","category":"page"},{"location":"examples/besselj/#","page":"Bessel function","title":"Bessel function","text":"Here, Grad(ibesselj) is a callable instance of type Grad{typeof(ibesselj)}}. The first parameter Val(1) indicates the first argument is the loss.","category":"page"},{"location":"examples/besselj/#","page":"Bessel function","title":"Bessel function","text":"To obtain second order gradients, one can Feed dual numbers to this gradient function.","category":"page"},{"location":"examples/besselj/#","page":"Bessel function","title":"Bessel function","text":"using ForwardDiff: Dual\n_, hxy, _, hxx = Grad(ibesselj)(Val(1), Dual(y, zero(y)), 2, Dual(x, one(x)))\nprintln(\"The hessian dy^2/dx^2 is $(grad(hxx).partials[1])\")","category":"page"},{"location":"examples/besselj/#","page":"Bessel function","title":"Bessel function","text":"Here, the gradient field is a Dual number, it has a field partials that stores the derivative with respect to x. This is the Hessian that we need.","category":"page"},{"location":"examples/besselj/#CUDA-programming-1","page":"Bessel function","title":"CUDA programming","text":"","category":"section"},{"location":"examples/besselj/#","page":"Bessel function","title":"Bessel function","text":"The AD in NiLang avoids most heap allocation, so that it is able to execute on a GPU device We suggest using KernelAbstraction, it provides compatibility between CPU and GPU. To execute the above function on GPU, we need only 11 lines of code.","category":"page"},{"location":"examples/besselj/#","page":"Bessel function","title":"Bessel function","text":"using CuArrays, GPUArrays, KernelAbstractions\n\n@i @kernel function bessel_kernel(out!, v, z)\n    @invcheckoff i ← @index(Global)\n    ibesselj(out![i], v, z[i])\n    @invcheckoff i → @index(Global)\nend","category":"page"},{"location":"examples/besselj/#","page":"Bessel function","title":"Bessel function","text":"We have a macro support to KernelAbstraction in NiLang. So it is possible to launch directly like.","category":"page"},{"location":"examples/besselj/#","page":"Bessel function","title":"Bessel function","text":"@i function befunc(out!, v::Integer, z)\n    @launchkernel CUDA() 256 length(out!) bessel_kernel(out!, v, z)\nend","category":"page"},{"location":"examples/besselj/#","page":"Bessel function","title":"Bessel function","text":"It is equivalent to call","category":"page"},{"location":"examples/besselj/#","page":"Bessel function","title":"Bessel function","text":"(~bessel_kernel)(CUDA(), 256)(out!, v, z; ndrange=length(out!))","category":"page"},{"location":"examples/besselj/#","page":"Bessel function","title":"Bessel function","text":"But it will execute the job eagerly for you. We will consider better support in the future.","category":"page"},{"location":"examples/besselj/#","page":"Bessel function","title":"Bessel function","text":"Except it is reversible","category":"page"},{"location":"examples/besselj/#","page":"Bessel function","title":"Bessel function","text":"julia> @code_reverse @launchkernel CUDA() 256 length(out!) bessel_kernel(out!, v, z)\n:(#= REPL[4]:1 =# @launchkernel CUDA() 256 length(out!) (~bessel_kernel)(out!, v, z))","category":"page"},{"location":"examples/besselj/#","page":"Bessel function","title":"Bessel function","text":"To test this function, we first define input parameters a and output out!","category":"page"},{"location":"examples/besselj/#","page":"Bessel function","title":"Bessel function","text":"a = CuArray(rand(128))\nout! = CuArray(zeros(128))","category":"page"},{"location":"examples/besselj/#","page":"Bessel function","title":"Bessel function","text":"We wrap the output with a randomly initialized gradient field, suppose we get the gradients from a virtual loss function. Also, we need to initialize an empty gradient field for elements in input cuda tensor a.","category":"page"},{"location":"examples/besselj/#","page":"Bessel function","title":"Bessel function","text":"out! = ibesselj(out!, 2, GVar.(a))[1]\nout_g! = GVar.(out!, CuArray(randn(128)))","category":"page"},{"location":"examples/besselj/#","page":"Bessel function","title":"Bessel function","text":"Call the inverse program, the multiple dispatch will drive you to the goal.","category":"page"},{"location":"examples/besselj/#","page":"Bessel function","title":"Bessel function","text":"(~ibesselj)(out_g!, 2, GVar.(a))","category":"page"},{"location":"examples/besselj/#","page":"Bessel function","title":"Bessel function","text":"You will get CUDA arrays with GVar elements as output, their gradient fields are what you want. Cheers! Now you have a adjoint mode differentiable CUDA kernel.","category":"page"},{"location":"examples/besselj/#Benchmark-1","page":"Bessel function","title":"Benchmark","text":"","category":"section"},{"location":"examples/besselj/#","page":"Bessel function","title":"Bessel function","text":"We have different source to souce automatic differention implementations of the first type Bessel function J_2(10) benchmarked and show the results below.","category":"page"},{"location":"examples/besselj/#","page":"Bessel function","title":"Bessel function","text":"Package Tangent/Adjoint T_rm min/ns Space/KB\nJulia - 22 0\nNiLang - 59 0\nForwardDiff Tangent 35 0\nManual Adjoint 83 0\nNiLang.AD Adjoint 213 0\nNiLang.AD (GPU) Adjoint 1.4 0\nZygote Adjoint 31201 13.47\nTapenade Adjoint ? ?","category":"page"},{"location":"examples/besselj/#","page":"Bessel function","title":"Bessel function","text":"Julia is the CPU time used for running the irreversible forward program, is the baseline of benchmarking. NiLang is the reversible implementation, it is 2.7 times slower than its irreversible counterpart. Here, we have remove the reversibility check. ForwardDiff gives the best performance because it is designed for functions with single input. It is even faster than manually derived gradients","category":"page"},{"location":"examples/besselj/#","page":"Bessel function","title":"Bessel function","text":"fracpartial J_nu(z)partial z = fracJ_nu-1 - J_nu+12","category":"page"},{"location":"examples/besselj/#","page":"Bessel function","title":"Bessel function","text":"NiLang.AD is the reversible differential programming implementation, it considers only the backward pass. The benchmark of its GPU version is estimated on Nvidia Titan V by broadcasting the gradient function on CUDA array of size 2^17 and take average. The Zygote benchmark considers both forward pass and backward pass. Tapenade is not yet ready.","category":"page"},{"location":"examples/besselj/#","page":"Bessel function","title":"Bessel function","text":"","category":"page"},{"location":"examples/besselj/#","page":"Bessel function","title":"Bessel function","text":"This page was generated using Literate.jl.","category":"page"},{"location":"grammar/#NiLang-Grammar-1","page":"NiLang Grammar","title":"NiLang Grammar","text":"","category":"section"},{"location":"grammar/#","page":"NiLang Grammar","title":"NiLang Grammar","text":"To define a reversible function one can use macro @i plus a function definition like bellow","category":"page"},{"location":"grammar/#","page":"NiLang Grammar","title":"NiLang Grammar","text":"\"\"\"\ndocstring...\n\"\"\"\n@i function f(args..., kwargs...) where {...}\n    <stmts>\nend","category":"page"},{"location":"grammar/#","page":"NiLang Grammar","title":"NiLang Grammar","text":"where the definition of <stmts> are shown in the grammar bellow. The following is a list of terminologies used in the definition of grammar","category":"page"},{"location":"grammar/#","page":"NiLang Grammar","title":"NiLang Grammar","text":"<ident>, symbols\n<num>, numbers\n0, empty statement\n<JuliaExpr>, native Julia expression\n[ ],  zero or one repetitions.","category":"page"},{"location":"grammar/#","page":"NiLang Grammar","title":"NiLang Grammar","text":"Here, all JuliaExpr should be pure, otherwise the reversibility is not guaranteed. Dataview is a view of a data, it can be a bijective mapping of an object, an item of an array or a field of an object.","category":"page"},{"location":"grammar/#","page":"NiLang Grammar","title":"NiLang Grammar","text":"Stmts : 0 \n      | Stmt\n      | Stmts Stmt\n      ;\n\nStmt : BlockStmt\n     | IfStmt\n     | WhileStmt\n     | ForStmt\n     | InstrStmt\n     | RevStmt\n     | AncillaStmt\n     | TypecastStmt \n     | @routine Stmt\n     | @safe <JuliaExpr>\n     | CallStmt\n     ;\n\n\nBlockStmt : 'begin' Stmts 'end';\n\nRevCond : '(' <JuliaExpr> ',' <JuliaExpr> ')';\n\nIfStmt : 'if' RevCond Stmts ['else' Stmts] 'end';\n\nWhileStmt : 'while' RevCond Stmts 'end';\n\nRange : <JuliaExpr> ':' <JuliaExpr> [':' <JuliaExpr>];\n\nForStmt : 'for' <ident> '=' Range Stmts 'end';\n\nKwArg : <ident> '=' <JuliaExpr>;\n\nKwArgs : [KwArgs ','] KwArg ;\n\nCallStmt : <JuliaExpr> '(' [DataViews] [';' KwArgs] ')';\n\nConstant : <num> | 'π';\n\nInstrBinOp : '+=' | '-=' | '⊻=';\n\nInstrTrailer : ['.'] '(' [DataViews] ')';\n\nInstrStmt : DataView InstrBinOp <ident> [InstrTrailer];\n\nRevStmt : '~' Stmt;\n\nAncillaStmt : <ident> '←' <JuliaExpr>\n            | <ident> '→' <JuliaExpr>\n            ;\n\nTypecastStmt : '(' <JuliaExpr> '=>' <JuliaExpr> ')' '(' <ident> ')';\n\n@routine : '@routine' <ident> Stmt;\n\n@safe : '@safe' <JuliaExpr>;\n\nDataViews : 0\n          | DataView\n          | DataViews ',' DataView\n          | DataViews ',' DataView '...'\n          ;\n\nDataView : DataView '[' <JuliaExpr> ']'\n         | DataView '.' <ident>\n         | DataView '|>' <JuliaExpr>\n         | DataView '\\''\n         | '-' DataView\n         | Constant\n         | <ident>\n         ;","category":"page"},{"location":"why/#What-is-Reversible-Computing-and-why-do-we-need-it-1","page":"What and Why","title":"What is Reversible Computing and why do we need it","text":"","category":"section"},{"location":"why/#what-is-reversible-computing-1","page":"What and Why","title":"what is reversible computing","text":"","category":"section"},{"location":"why/#","page":"What and Why","title":"What and Why","text":"Reversible computing is a computing paradigm that can deterministically undo all the previous changes, which requires user not erasing any information during computations. It boomed during 1970-2005, however, but runs into a winter after that. The following book covers most of reversible computing that you want to know, especially the software part.","category":"page"},{"location":"why/#","page":"What and Why","title":"What and Why","text":"(Image: Introduction to Reversible Computing)","category":"page"},{"location":"why/#Why-reversible-computing-is-the-future-of-computing:-a-physicist's-perspective-1","page":"What and Why","title":"Why reversible computing is the future of computing: a physicist's perspective","text":"","category":"section"},{"location":"why/#","page":"What and Why","title":"What and Why","text":"Reversible computing can do anything traditional computing can do. It can be simulated on a irreversible device, but sometimes need either more space or time or both. So, why are people still interested in reversible computing?","category":"page"},{"location":"why/#From-the-energy-perspective-1","page":"What and Why","title":"From the energy perspective","text":"","category":"section"},{"location":"why/#","page":"What and Why","title":"What and Why","text":"Energy is one of the most important bottleneck of computation. Energy efficiency of computing devices affect the value of bitcoins, user experiences of your cell phone, artificial intelligence (AI) industry.","category":"page"},{"location":"why/#","page":"What and Why","title":"What and Why","text":"Reversible computing devices are more energy efficiency, citing the famous Landauer's principle","category":"page"},{"location":"why/#","page":"What and Why","title":"What and Why","text":"Landauer's principle is a physical principle pertaining to the lower theoretical limit of energy consumption of computation. It holds that \"any logically irreversible manipulation of information, such as the erasure of a bit or the merging of two computation paths, must be accompanied by a corresponding entropy increase in non-information-bearing degrees of freedom of the information-processing apparatus or its environment\".Another way of phrasing Landauer's principle is that if an observer loses information about a physical system, the observer loses the ability to extract work from that system.","category":"page"},{"location":"why/#","page":"What and Why","title":"What and Why","text":"In the future, the building block of information technology is probably based on microscopic dynamics (e.g. cold atoms, DNA, quantum dot). Irreversibility is rare in these systems. Irreversible dynamics is available only in macroscopic world, where you assume the existence an infinite sized \"bath\". For example, Like \"measure\" operation in quantum computing (a kind of reversible computing) is irreversible, as well as one of the slowest operation on quantum devices, one have to wait for the read out signal.","category":"page"},{"location":"why/#","page":"What and Why","title":"What and Why","text":"However, investors lost interest to reversible computing at around 2005 according to this paper because energy efficiency of traditional CMOS is still approximately 2 orders above the Landauer's limit, there should still be a lot room to improve, while many reversible computing devices are not \"technical smooth\". ","category":"page"},{"location":"why/#","page":"What and Why","title":"What and Why","text":"Undoubtedly, traditional CMOS comes into the bottleneck of energy efficiency recent years. The reversible computing scheme adiabatic CMOS is technical smooth and shows orders more energy efficient than traditional CMOS, and it is already useful in spacecrafts. The detailed analysis of the energy-speed trade off in adiabatic CMOS can be found here.","category":"page"},{"location":"why/#From-the-artificial-intelligence-perspective-1","page":"What and Why","title":"From the artificial intelligence perspective","text":"","category":"section"},{"location":"why/#","page":"What and Why","title":"What and Why","text":"In reversible programming, differentiable programming is directly achievable. Notice most differentiable programming are built up of basic instructions like \"+\", \"*\", \"/\", \"-\". We can use these basic instructions to write Bessel functions, singular value decompositions et. al. Reversible programming allows you to define adjoint rules on instructions only, rather than defining a lot primitives. This timing is perfect because at this timing, AI is very popular with a lot amazing applications. It requires reversibility for AD, and it is also power consuming. In the past, most source to source AD frameworks are based checkpointing. Checkpointing is a naive version of reversible programming that caches everything into a global stack. Reversible programming provides us more flexibility.","category":"page"},{"location":"why/#Embrace-Reversible-Computing:-Software-goes-first-1","page":"What and Why","title":"Embrace Reversible Computing: Software goes first","text":"","category":"section"},{"location":"why/#","page":"What and Why","title":"What and Why","text":"Reversible hardwares relies on reversible software and compiling techniques.","category":"page"},{"location":"why/#","page":"What and Why","title":"What and Why","text":"The inverse is not true. Reversible hardware is an energy efficient host for reversible programs.","category":"page"},{"location":"why/#","page":"What and Why","title":"What and Why","text":"Reversible programming is already useful in machine learning, it is technical smooth to embrace reversible computing from the software side.","category":"page"},{"location":"why/#FAQ-1","page":"What and Why","title":"FAQ","text":"","category":"section"},{"location":"why/#","page":"What and Why","title":"What and Why","text":"Q: does this compose with cudanative kernels? So we don't have to write custom adjoints?","category":"page"},{"location":"why/#","page":"What and Why","title":"What and Why","text":"A: It is composible with KernelAbstraction, we have an example here For CUDAnative, the problem is the power operations ^ on GPU is not compatible with that on CPU. It can be solved, but needs some patch.","category":"page"},{"location":"why/#","page":"What and Why","title":"What and Why","text":"Still, I want to emphasis writing differentiable parallel kernels have the problem of share read in forward will become shared write when back propagating gradients, which produces wrong gradients. It is a known hard problem in combining CUDA programming and differential programming.","category":"page"},{"location":"examples/port_zygote/#","page":"How to port NiLang to Zygote","title":"How to port NiLang to Zygote","text":"EditURL = \"https://github.com/GiggleLiu/NiLang.jl/blob/master/examples/port_zygote.jl\"","category":"page"},{"location":"examples/port_zygote/#How-to-port-NiLang-to-Zygote-1","page":"How to port NiLang to Zygote","title":"How to port NiLang to Zygote","text":"","category":"section"},{"location":"examples/port_zygote/#","page":"How to port NiLang to Zygote","title":"How to port NiLang to Zygote","text":"using NiLang, NiLang.AD, Zygote","category":"page"},{"location":"examples/port_zygote/#","page":"How to port NiLang to Zygote","title":"How to port NiLang to Zygote","text":"Let's start from the Julia native implementation of norm2 function.","category":"page"},{"location":"examples/port_zygote/#","page":"How to port NiLang to Zygote","title":"How to port NiLang to Zygote","text":"function norm2(x::AbstractArray{T}) where T\n    out = zero(T)\n    for i=1:length(x)\n        @inbounds out += x[i]^2\n    end\n    return out\nend","category":"page"},{"location":"examples/port_zygote/#","page":"How to port NiLang to Zygote","title":"How to port NiLang to Zygote","text":"Zygote is able to generate correct gradients, but much slower than the original program.","category":"page"},{"location":"examples/port_zygote/#","page":"How to port NiLang to Zygote","title":"How to port NiLang to Zygote","text":"using BenchmarkTools\nx = randn(1000);\noriginal_grad = norm2'(x)\n@benchmark norm2'($x) seconds=1","category":"page"},{"location":"examples/port_zygote/#","page":"How to port NiLang to Zygote","title":"How to port NiLang to Zygote","text":"The orignal program is","category":"page"},{"location":"examples/port_zygote/#","page":"How to port NiLang to Zygote","title":"How to port NiLang to Zygote","text":"@benchmark norm2($x) seconds=1","category":"page"},{"location":"examples/port_zygote/#","page":"How to port NiLang to Zygote","title":"How to port NiLang to Zygote","text":"Then we have the reversible implementation","category":"page"},{"location":"examples/port_zygote/#","page":"How to port NiLang to Zygote","title":"How to port NiLang to Zygote","text":"@i function r_norm2(out::T, x::AbstractArray{T}) where T\n    for i=1:length(x)\n        @inbounds out += x[i]^2\n    end\nend","category":"page"},{"location":"examples/port_zygote/#","page":"How to port NiLang to Zygote","title":"How to port NiLang to Zygote","text":"The gradient generated by NiLang is much faster, which is comparable to the forward program","category":"page"},{"location":"examples/port_zygote/#","page":"How to port NiLang to Zygote","title":"How to port NiLang to Zygote","text":"@benchmark (~r_norm2)(GVar($(norm2(x)), 1.0), $(GVar(x))) seconds=1","category":"page"},{"location":"examples/port_zygote/#","page":"How to port NiLang to Zygote","title":"How to port NiLang to Zygote","text":"to enjoy the speed of NiLang in Zygote, just bind the adjoint rule","category":"page"},{"location":"examples/port_zygote/#","page":"How to port NiLang to Zygote","title":"How to port NiLang to Zygote","text":"Zygote.@adjoint function norm2(x::AbstractArray{T}) where T\n    out = norm2(x)\n    out, δy -> (grad((~r_norm2)(GVar(out, δy), GVar(x))[2]),)\nend\n@assert norm2'(x) ≈ original_grad","category":"page"},{"location":"examples/port_zygote/#","page":"How to port NiLang to Zygote","title":"How to port NiLang to Zygote","text":"See, much faster","category":"page"},{"location":"examples/port_zygote/#","page":"How to port NiLang to Zygote","title":"How to port NiLang to Zygote","text":"@benchmark norm2'(x) seconds=1","category":"page"},{"location":"examples/port_zygote/#","page":"How to port NiLang to Zygote","title":"How to port NiLang to Zygote","text":"","category":"page"},{"location":"examples/port_zygote/#","page":"How to port NiLang to Zygote","title":"How to port NiLang to Zygote","text":"This page was generated using Literate.jl.","category":"page"},{"location":"instructions/#Instruction-Reference-1","page":"Instruction Reference","title":"Instruction Reference","text":"","category":"section"},{"location":"instructions/#Instruction-definitions-1","page":"Instruction Reference","title":"Instruction definitions","text":"","category":"section"},{"location":"instructions/#","page":"Instruction Reference","title":"Instruction Reference","text":"The Julia functions and symbols for instructions","category":"page"},{"location":"instructions/#","page":"Instruction Reference","title":"Instruction Reference","text":"instruction translated symbol\ny mathrel+= f(args) PlusEq(f)(args...) oplus\ny mathrel-= f(args) MinusEq(f)(args...) ominus\ny mathrelveebar= f(args) \\texttt{XorEq(f)(args...) odot","category":"page"},{"location":"instructions/#","page":"Instruction Reference","title":"Instruction Reference","text":"The list of reversible instructions that implemented in NiLang","category":"page"},{"location":"instructions/#","page":"Instruction Reference","title":"Instruction Reference","text":"instruction output\nrm SWAP(a b) b a\nrm ROT(a b theta) a costheta - bsintheta b costheta + asintheta theta\nrm IROT(a b theta) a costheta + bsintheta b costheta - asintheta theta\ny mathrel+= a^wedge b y+a^b a b\ny mathrel+= exp(x) y+e^x x\ny mathrel+= log(x) y+log x x\ny mathrel+= sin(x) y+sin x x\ny mathrel+= cos(x) y+cos x x\ny mathrel+= rm abs(x) $y+\n-(y) -y\nrm CONJ(y) y","category":"page"},{"location":"instructions/#","page":"Instruction Reference","title":"Instruction Reference","text":"\".\" is the broadcasting operations in Julia.","category":"page"},{"location":"instructions/#Jacobians-and-Hessians-for-Instructions-1","page":"Instruction Reference","title":"Jacobians and Hessians for Instructions","text":"","category":"section"},{"location":"instructions/#","page":"Instruction Reference","title":"Instruction Reference","text":"See my blog post.","category":"page"},{"location":"examples/nice/#","page":"NICE network","title":"NICE network","text":"EditURL = \"https://github.com/GiggleLiu/NiLang.jl/blob/master/examples/nice.jl\"","category":"page"},{"location":"examples/nice/#NICE-network-1","page":"NICE network","title":"NICE network","text":"","category":"section"},{"location":"examples/nice/#","page":"NICE network","title":"NICE network","text":"For the definition of this network and concepts of normalizing flow, please refer this nice blog: https://lilianweng.github.io/lil-log/2018/10/13/flow-based-deep-generative-models.html, and the pytorch notebook: https://github.com/GiggleLiu/marburg/blob/master/notebooks/nice.ipynb","category":"page"},{"location":"examples/nice/#","page":"NICE network","title":"NICE network","text":"using NiLang, NiLang.AD\nusing LinearAlgebra\nusing DelimitedFiles\nusing Plots","category":"page"},{"location":"examples/nice/#","page":"NICE network","title":"NICE network","text":"include the optimizer, you can find it under the Adam.jl file in the examples/ folder.","category":"page"},{"location":"examples/nice/#","page":"NICE network","title":"NICE network","text":"include(\"Adam.jl\")","category":"page"},{"location":"examples/nice/#Model-definition-1","page":"NICE network","title":"Model definition","text":"","category":"section"},{"location":"examples/nice/#","page":"NICE network","title":"NICE network","text":"First, define the single layer transformation and its behavior under GVar - the gradient wrapper.","category":"page"},{"location":"examples/nice/#","page":"NICE network","title":"NICE network","text":"struct NiceLayer{T}\n    W1::Matrix{T}\n    b1::Vector{T}\n    W2::Matrix{T}\n    b2::Vector{T}\n    y1::Vector{T}\n    y1a::Vector{T}\nend\nNiLang.AD.GVar(x::NiceLayer) = NiceLayer(GVar(x.W1), GVar(x.b1), GVar(x.W2), GVar(x.b2), GVar(x.y1), GVar(x.y1a))\n\n\"\"\"Apply a single NICE transformation.\"\"\"\n@i function nice_layer!(x::AbstractVector{T}, layer::NiceLayer{T},\n                y!::AbstractVector{T}) where T\n    @routine @invcheckoff begin\n        i_affine!(layer.y1, layer.W1, layer.b1, x)\n        @inbounds for i=1:length(layer.y1)\n            if (layer.y1[i] > 0, ~)\n                layer.y1a[i] += layer.y1[i]\n            end\n        end\n    end\n    i_affine!(y!, layer.W2, layer.b2, layer.y1a)\n    ~@routine\n    # clean up accumulated rounding error, since this memory is reused.\n    @safe layer.y1 .= zero(T)\nend","category":"page"},{"location":"examples/nice/#","page":"NICE network","title":"NICE network","text":"Here, in each layer, we use the information in x to update y!. During computing, we use the y1 and y1a fields of the network as ancilla space, both of them can be uncomputed at the end of the function. However, we need to erase small numbers to make sure the rounding error does not accumulate.","category":"page"},{"location":"examples/nice/#","page":"NICE network","title":"NICE network","text":"A nice network always transforms inputs reversibly. We update one half of x! a time, so that input and output memory space do not clash.","category":"page"},{"location":"examples/nice/#","page":"NICE network","title":"NICE network","text":"const NiceNetwork{T} = Vector{NiceLayer{T}}\n\n\"\"\"Apply a the whole NICE network.\"\"\"\n@i function nice_network!(x!::AbstractVector{T}, network::NiceNetwork{T}) where T\n    @invcheckoff for i=1:length(network)\n        np ← length(x!)\n        if (i%2==0, ~)\n            @inbounds nice_layer!(view(x!,np÷2+1:np), network[i], view(x!,1:np÷2))\n        else\n            @inbounds nice_layer!(view(x!,1:np÷2), network[i], view(x!,np÷2+1:np))\n        end\n    end\nend\n\nfunction random_nice_network(nparams::Int, nhidden::Int, nlayer::Int; scale=0.1)\n    random_nice_network(Float64, nparams, nhidden, nlayer; scale=scale)\nend\n\nfunction random_nice_network(::Type{T}, nparams::Int, nhidden::Int, nlayer::Int; scale=0.1) where T\n    nin = nparams÷2\n    scale = T(scale)\n    y1 = zeros(T, nhidden)\n    NiceLayer{T}[NiceLayer(randn(T, nhidden, nin)*scale, randn(T, nhidden)*scale,\n            randn(T, nin, nhidden)*scale, randn(T, nin)*scale, y1, zero(y1)) for _ = 1:nlayer]\nend","category":"page"},{"location":"examples/nice/#Parameter-management-1","page":"NICE network","title":"Parameter management","text":"","category":"section"},{"location":"examples/nice/#","page":"NICE network","title":"NICE network","text":"nparameters(n::NiceLayer) = length(n.W1) + length(n.b1) + length(n.W2) + length(n.b2)\nnparameters(n::NiceNetwork) = sum(nparameters, n)\n\n\"\"\"collect parameters in the `layer` into a vector `out`.\"\"\"\nfunction collect_params!(out, layer::NiceLayer)\n    a, b, c, d = length(layer.W1), length(layer.b1), length(layer.W2), length(layer.b2)\n    out[1:a] .= vec(layer.W1)\n    out[a+1:a+b] .= layer.b1\n    out[a+b+1:a+b+c] .= vec(layer.W2)\n    out[a+b+c+1:end] .= layer.b2\n    return out\nend\n\n\"\"\"dispatch vectorized parameters `out` into the `layer`.\"\"\"\nfunction dispatch_params!(layer::NiceLayer, out)\n    a, b, c, d = length(layer.W1), length(layer.b1), length(layer.W2), length(layer.b2)\n    vec(layer.W1) .= out[1:a]\n    layer.b1 .= out[a+1:a+b]\n    vec(layer.W2) .= out[a+b+1:a+b+c]\n    layer.b2 .= out[a+b+c+1:end]\n    return layer\nend\n\nfunction collect_params(n::NiceNetwork{T}) where T\n    out = zeros(T, nparameters(n))\n    k = 0\n    for layer in n\n        np = nparameters(layer)\n        collect_params!(view(out, k+1:k+np), layer)\n        k += np\n    end\n    return out\nend\n\nfunction dispatch_params!(network::NiceNetwork, out)\n    k = 0\n    for layer in network\n        np = nparameters(layer)\n        dispatch_params!(layer, view(out, k+1:k+np))\n        k += np\n    end\n    return network\nend","category":"page"},{"location":"examples/nice/#Loss-function-1","page":"NICE network","title":"Loss function","text":"","category":"section"},{"location":"examples/nice/#","page":"NICE network","title":"NICE network","text":"To obtain the log-probability of a data.","category":"page"},{"location":"examples/nice/#","page":"NICE network","title":"NICE network","text":"@i function logp!(out!::T, x!::AbstractVector{T}, network::NiceNetwork{T}) where T\n    (~nice_network!)(x!, network)\n    @invcheckoff for i = 1:length(x!)\n        @routine begin\n            xsq ← zero(T)\n            @inbounds xsq += x![i]^2\n        end\n        out! -= 0.5 * xsq\n        ~@routine\n    end\nend","category":"page"},{"location":"examples/nice/#","page":"NICE network","title":"NICE network","text":"The negative-log-likelihood loss function","category":"page"},{"location":"examples/nice/#","page":"NICE network","title":"NICE network","text":"@i function nice_nll!(out!::T, cum!::T, xs!::Matrix{T}, network::NiceNetwork{T}) where T\n    @invcheckoff for i=1:size(xs!, 2)\n        @inbounds logp!(cum!, view(xs!,:,i), network)\n    end\n    out! -= cum!/size(xs!, 2)\nend","category":"page"},{"location":"examples/nice/#Training-1","page":"NICE network","title":"Training","text":"","category":"section"},{"location":"examples/nice/#","page":"NICE network","title":"NICE network","text":"function train(x_data, model; num_epochs = 800)\n    num_vars = size(x_data, 1)\n    params = collect_params(model)\n    optimizer = Adam(; lr=0.01)\n    for epoch = 1:num_epochs\n        loss, a, b, c = nice_nll!(0.0, 0.0, copy(x_data), model)\n        if epoch % 50 == 1\n            println(\"epoch = $epoch, loss = $loss\")\n            display(showmodel(x_data, model))\n        end\n        _, _, _, gmodel = (~nice_nll!)(GVar(loss, 1.0), GVar(a), GVar(b), GVar(c))\n        g = grad.(collect_params(gmodel))\n        update!(params, grad.(collect_params(gmodel)), optimizer)\n        dispatch_params!(model, params)\n    end\n    return model\nend\n\nfunction showmodel(x_data, model; nsamples=2000)\n    scatter(x_data[1,1:nsamples], x_data[2,1:nsamples]; xlims=(-5,5), ylims=(-5,5))\n    zs = randn(2, nsamples)\n    for i=1:nsamples\n        nice_network!(view(zs, :, i), model)\n    end\n    scatter!(zs[1,:], zs[2,:])\nend","category":"page"},{"location":"examples/nice/#","page":"NICE network","title":"NICE network","text":"you can find the training data in examples/ folder","category":"page"},{"location":"examples/nice/#","page":"NICE network","title":"NICE network","text":"x_data = Matrix(readdlm(joinpath(@__DIR__, \"train.dat\"))')\n\nimport Random; Random.seed!(22)\nmodel = random_nice_network(Float64, size(x_data, 1), 10, 4; scale=0.1)","category":"page"},{"location":"examples/nice/#","page":"NICE network","title":"NICE network","text":"Before training, the distribution looks like (Image: before)","category":"page"},{"location":"examples/nice/#","page":"NICE network","title":"NICE network","text":"model = train(x_data, model; num_epochs=800)","category":"page"},{"location":"examples/nice/#","page":"NICE network","title":"NICE network","text":"After training, the distribution looks like (Image: before)","category":"page"},{"location":"examples/nice/#","page":"NICE network","title":"NICE network","text":"","category":"page"},{"location":"examples/nice/#","page":"NICE network","title":"NICE network","text":"This page was generated using Literate.jl.","category":"page"},{"location":"#NiLang.jl-1","page":"Home","title":"NiLang.jl","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"NiLang is a reversible eDSL that can run backwards. The motation is to support source to source AD.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Check our paper!","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Welcome for discussion in Julia slack, #autodiff and #reversible-commputing channel.","category":"page"},{"location":"#Tutorials-1","page":"Home","title":"Tutorials","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"Pages = [\n    \"tutorial.md\",\n    \"examples/port_zygote.md\",\n]\nDepth = 1","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Also see blog posts","category":"page"},{"location":"#","page":"Home","title":"Home","text":"How to write a program differentiably\nSimulate a reversible Turing machine in 50 lines of code","category":"page"},{"location":"#Examples-1","page":"Home","title":"Examples","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"Pages = [\n    \"examples/fib.md\",\n    \"examples/besselj.md\",\n    \"examples/sparse.md\",\n    \"examples/lognumber.md\",\n    \"examples/unitary.md\",\n    \"examples/qr.md\",\n    \"examples/nice.md\",\n    \"examples/realnvp.md\",\n    \"examples/boxmuller.md\",\n]\nDepth = 1","category":"page"},{"location":"#Manual-1","page":"Home","title":"Manual","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"Pages = [\n    \"grammar.md\",\n    \"instructions.md\",\n    \"extend.md\",\n    \"examples/sharedwrite.md\",\n    \"api.md\",\n]\nDepth = 2","category":"page"},{"location":"tutorial/#My-first-NiLang-program-1","page":"My first NiLang program","title":"My first NiLang program","text":"","category":"section"},{"location":"tutorial/#Basic-Statements-1","page":"My first NiLang program","title":"Basic Statements","text":"","category":"section"},{"location":"tutorial/#","page":"My first NiLang program","title":"My first NiLang program","text":"Statement Meaning\nx ← val allocate a new variable x, with an initial value val (a constant).\nx → val deallocate variable x with content val.\nx += f(y) a reversible instruction.\nx .+= f.(y) instruction call with broadcasting.\nf(y) a reversible function.\nf.(y) function call with broadcasting.\nif (pre, post) ... end if statement.\nwhile (pre, post) ... end while statement.\nfor x=1:3 ... end for statement.\nbegin ... end block statement.\n@safe ... insert an irreversible statement.\n~(...) inverse a statement.\n@routine ... record a routine in the routine stack.\n~@routine place the inverse of the routine on routine stack top.","category":"page"},{"location":"tutorial/#","page":"My first NiLang program","title":"My first NiLang program","text":"The condition expression in if and while statements are a bit hard to digest, please refer our paper arXiv:2003.04617.","category":"page"},{"location":"tutorial/#A-reversible-program-1","page":"My first NiLang program","title":"A reversible program","text":"","category":"section"},{"location":"tutorial/#","page":"My first NiLang program","title":"My first NiLang program","text":"Our first program is to compute a loss function defined as","category":"page"},{"location":"tutorial/#","page":"My first NiLang program","title":"My first NiLang program","text":"mathcalL = vec z^T(avecx + vecy)","category":"page"},{"location":"tutorial/#","page":"My first NiLang program","title":"My first NiLang program","text":"where vec x, vec y and vecz are column vectors, a is a scalar.","category":"page"},{"location":"tutorial/#","page":"My first NiLang program","title":"My first NiLang program","text":"@i function r_axpy!(a::T, x::AbstractVector{T}, y!::AbstractVector{T}) where T\n    @safe @assert length(x) == length(y!)\n    for i=1:length(x)\n        y![i] += a * x[i]\n    end\nend\n\n@i function r_loss(out!, a, x, y!, z)\n    r_axpy!(a, x, y!)\n    for i=1:length(z)\n    \tout! += z[i] * y![i]\n    end\nend","category":"page"},{"location":"tutorial/#","page":"My first NiLang program","title":"My first NiLang program","text":"Functions do not have return statements, they return input arguments instead. Hence r_loss defines a 5 variable to 5 variable bijection. Let's check the reversibility","category":"page"},{"location":"tutorial/#","page":"My first NiLang program","title":"My first NiLang program","text":"julia> out, a, x, y, z = 0.0, 2.0, randn(3), randn(3), randn(3)\n(0.0, 2.0, [0.9265845776642722, 0.8532458027149912, 0.6201064385679095],\n [1.1142808415540468, 0.5506163710455121, -1.9873779917908814],\n [1.1603953198942412, 0.5562855137395296, 1.9650050430758796])\n\njulia> out, a, x, y, z = r_loss(out, a, x, y, z)\n(3.2308283403544342, 2.0, [0.9265845776642722, 0.8532458027149912, 0.6201064385679095],\n [2.967449996882591, 2.2571079764754947, -0.7471651146550624],\n [1.1603953198942412, 0.5562855137395296, 1.9650050430758796])","category":"page"},{"location":"tutorial/#","page":"My first NiLang program","title":"My first NiLang program","text":"We find the contents in out and y are changed after calling the loss function. Then we call the inverse loss function ~r_loss.","category":"page"},{"location":"tutorial/#","page":"My first NiLang program","title":"My first NiLang program","text":"julia> out, a, x, y, z = (~r_loss)(out, a, x, y, z)\n(0.0, 2.0, [0.9265845776642722, 0.8532458027149912, 0.6201064385679095],\n [1.1142808415540466, 0.5506163710455123, -1.9873779917908814],\n [1.1603953198942412, 0.5562855137395296, 1.9650050430758796])","category":"page"},{"location":"tutorial/#","page":"My first NiLang program","title":"My first NiLang program","text":"Values are restored. Here, instead of assigning variables one by one, one can also use the macro @instr","category":"page"},{"location":"tutorial/#","page":"My first NiLang program","title":"My first NiLang program","text":"@instr r_loss(out, a, x, y, z)","category":"page"},{"location":"tutorial/#","page":"My first NiLang program","title":"My first NiLang program","text":"@instr macro is for executing a reversible statement.","category":"page"},{"location":"tutorial/#My-first-reversible-AD-program-1","page":"My first NiLang program","title":"My first reversible AD program","text":"","category":"section"},{"location":"tutorial/#","page":"My first NiLang program","title":"My first NiLang program","text":"julia> using NiLang.AD: Grad\n\njulia> x, y, z = randn(3), randn(3), randn(3)\n([2.2683181471139906, -0.7374245775047469, 0.9568936661385092],\n [1.0275914704043452, 1.647972121962081, -0.8349079845797637],\n [1.4272076815911372, 0.5317755971532034, 0.4412421572457776])\n\njulia> Grad(r_loss)(0.0, 0.5, x, y, z; iloss=1)\n(GVar(0.0, 1.0), GVar(0.5, 3.2674385142974036),\n GVar{Float64,Float64}[GVar(2.2683181471139906, 0.7136038407955686), GVar(-0.7374245775047469, 0.2658877985766017), GVar(0.9568936661385092, 0.2206210786228888)],\n GVar{Float64,Float64}[GVar(2.1617505439613405, 1.4272076815911372), GVar(1.2792598332097076, 0.5317755971532034), GVar(-0.35646115151050906, 0.4412421572457776)],\n GVar{Float64,Float64}[GVar(1.4272076815911372, 3.295909617518336), GVar(0.5317755971532034, 0.9105475444573341), GVar(0.4412421572457776, 0.12198568155874556)])\n\njulia> gout, ga, gx, gy, gz = Grad(r_loss)(0.0, 0.5, x, y, z; iloss=1)\n(GVar(0.0, 1.0), GVar(0.5, 3.2674385142974036),\n GVar{Float64,Float64}[GVar(2.2683181471139906, 0.7136038407955686), GVar(-0.7374245775047469, 0.2658877985766017), GVar(0.9568936661385092, 0.2206210786228888)],\n GVar{Float64,Float64}[GVar(3.295909617518336, 1.4272076815911372), GVar(0.9105475444573341, 0.5317755971532034), GVar(0.12198568155874556, 0.4412421572457776)],\n GVar{Float64,Float64}[GVar(1.4272076815911372, 4.4300686910753315), GVar(0.5317755971532034, 0.5418352557049606), GVar(0.4412421572457776, 0.6004325146280002)])","category":"page"},{"location":"tutorial/#","page":"My first NiLang program","title":"My first NiLang program","text":"The results are a bit messy, since NiLang wraps each element with a gradient field automatically. We can take the gradient field using the grad function like","category":"page"},{"location":"tutorial/#","page":"My first NiLang program","title":"My first NiLang program","text":"julia> grad(gout)\n1.0\n\njulia> grad(ga)\n3.2674385142974036\n\njulia> grad(gx)\n3-element Array{Float64,1}:\n 0.7136038407955686\n 0.2658877985766017\n 0.2206210786228888\n\njulia> grad(gy)\n3-element Array{Float64,1}:\n 1.4272076815911372\n 0.5317755971532034\n 0.4412421572457776\n\njulia> grad(gz)\n3-element Array{Float64,1}:\n 4.4300686910753315\n 0.5418352557049606\n 0.6004325146280002","category":"page"},{"location":"examples/realnvp/#","page":"RealNVP network","title":"RealNVP network","text":"EditURL = \"https://github.com/GiggleLiu/NiLang.jl/blob/master/examples/realnvp.jl\"","category":"page"},{"location":"examples/realnvp/#RealNVP-network-1","page":"RealNVP network","title":"RealNVP network","text":"","category":"section"},{"location":"examples/realnvp/#","page":"RealNVP network","title":"RealNVP network","text":"For the definition of this network and concepts of normalizing flow, please refer this realnvp blog: https://lilianweng.github.io/lil-log/2018/10/13/flow-based-deep-generative-models.html, and the pytorch notebook: https://github.com/GiggleLiu/marburg/blob/master/solutions/realnvp.ipynb","category":"page"},{"location":"examples/realnvp/#","page":"RealNVP network","title":"RealNVP network","text":"using NiLang, NiLang.AD\nusing LinearAlgebra\nusing DelimitedFiles\nusing Plots","category":"page"},{"location":"examples/realnvp/#","page":"RealNVP network","title":"RealNVP network","text":"include the optimizer, you can find it under the Adam.jl file in the examples/ folder.","category":"page"},{"location":"examples/realnvp/#","page":"RealNVP network","title":"RealNVP network","text":"include(\"Adam.jl\")","category":"page"},{"location":"examples/realnvp/#Model-definition-1","page":"RealNVP network","title":"Model definition","text":"","category":"section"},{"location":"examples/realnvp/#","page":"RealNVP network","title":"RealNVP network","text":"First, define the single layer transformation and its behavior under GVar - the gradient wrapper.","category":"page"},{"location":"examples/realnvp/#","page":"RealNVP network","title":"RealNVP network","text":"struct RealNVPLayer{T}\n    # transform network\n    W1::Matrix{T}\n    b1::Vector{T}\n    W2::Matrix{T}\n    b2::Vector{T}\n    y1::Vector{T}\n    y1a::Vector{T}\n\n    # scaling network\n    sW1::Matrix{T}\n    sb1::Vector{T}\n    sW2::Matrix{T}\n    sb2::Vector{T}\n    sy1::Vector{T}\n    sy1a::Vector{T}\nend\n\n\"\"\"collect parameters in the `layer` into a vector `out`.\"\"\"\nfunction collect_params!(out, layer::RealNVPLayer)\n    k=0\n    for field in [:W1, :b1, :W2, :b2, :sW1, :sb1, :sW2, :sb2]\n        v = getfield(layer, field)\n        nv = length(v)\n        out[k+1:k+nv] .= vec(v)\n        k += nv\n    end\n    return out\nend\n\n\"\"\"dispatch vectorized parameters `out` into the `layer`.\"\"\"\nfunction dispatch_params!(layer::RealNVPLayer, out)\n    k=0\n    for field in [:W1, :b1, :W2, :b2, :sW1, :sb1, :sW2, :sb2]\n        v = getfield(layer, field)\n        nv = length(v)\n        vec(v) .= out[k+1:k+nv]\n        k += nv\n    end\n    return out\nend\n\nfunction nparameters(n::RealNVPLayer)\n    sum(x->length(getfield(n, x)), [:W1, :b1, :W2, :b2, :sW1, :sb1, :sW2, :sb2])\nend","category":"page"},{"location":"examples/realnvp/#","page":"RealNVP network","title":"RealNVP network","text":"Then, we define network and how to access the parameters.","category":"page"},{"location":"examples/realnvp/#","page":"RealNVP network","title":"RealNVP network","text":"const RealNVP{T} = Vector{RealNVPLayer{T}}\n\nnparameters(n::RealNVP) = sum(nparameters, n)\n\nfunction collect_params(n::RealNVP{T}) where T\n    out = zeros(T, nparameters(n))\n    k = 0\n    for layer in n\n        np = nparameters(layer)\n        collect_params!(view(out, k+1:k+np), layer)\n        k += np\n    end\n    return out\nend\n\nfunction dispatch_params!(network::RealNVP, out)\n    k = 0\n    for layer in network\n        np = nparameters(layer)\n        dispatch_params!(layer, view(out, k+1:k+np))\n        k += np\n    end\n    return network\nend\n\nfunction random_realnvp(nparams::Int, nhidden::Int, nhidden_s::Int, nlayer::Int; scale=0.1)\n    random_realnvp(Float64, nparams, nhidden, nhidden_s::Int, nlayer; scale=scale)\nend\n\nfunction random_realnvp(::Type{T}, nparams::Int, nhidden::Int, nhidden_s::Int, nlayer::Int; scale=0.1) where T\n    nin = nparams÷2\n    scale = T(scale)\n    y1 = zeros(T, nhidden)\n    sy1 = zeros(T, nhidden_s)\n    RealNVPLayer{T}[RealNVPLayer(\n            randn(T, nhidden, nin)*scale, randn(T, nhidden)*scale,\n            randn(T, nin, nhidden)*scale, randn(T, nin)*scale, y1, zero(y1),\n            randn(T, nhidden_s, nin)*scale, randn(T, nhidden_s)*scale,\n            randn(T, nin, nhidden_s)*scale, randn(T, nin)*scale, sy1, zero(sy1),\n            ) for _ = 1:nlayer]\nend","category":"page"},{"location":"examples/realnvp/#Loss-function-1","page":"RealNVP network","title":"Loss function","text":"","category":"section"},{"location":"examples/realnvp/#","page":"RealNVP network","title":"RealNVP network","text":"In each layer, we use the information in x to update y!. During computing, we use to vector type ancillas y1 and y1a, both of them can be uncomputed at the end of the function.","category":"page"},{"location":"examples/realnvp/#","page":"RealNVP network","title":"RealNVP network","text":"@i function onelayer!(x::AbstractVector{T}, layer::RealNVPLayer{T},\n                y!::AbstractVector{T}, logjacobian!::T; islast) where T\n    @routine @invcheckoff begin\n        # scale network\n        scale ← zero(y!)\n        ytemp2 ← zero(y!)\n        i_affine!(layer.sy1, layer.sW1, layer.sb1, x)\n        @inbounds for i=1:length(layer.sy1)\n            if (layer.sy1[i] > 0, ~)\n                layer.sy1a[i] += layer.sy1[i]\n            end\n        end\n        i_affine!(scale, layer.sW2, layer.sb2, layer.sy1a)\n\n        # transform network\n        i_affine!(layer.y1, layer.W1, layer.b1, x)\n        # relu\n        @inbounds for i=1:length(layer.y1)\n            if (layer.y1[i] > 0, ~)\n                layer.y1a[i] += layer.y1[i]\n            end\n        end\n    end\n    # inplace multiply exp of scale! -- dangerous\n    @inbounds @invcheckoff for i=1:length(scale)\n        @routine begin\n            expscale ← zero(T)\n            tanhscale ← zero(T)\n            if (islast, ~)\n                tanhscale += tanh(scale[i])\n            else\n                tanhscale += scale[i]\n            end\n            expscale += exp(tanhscale)\n        end\n        logjacobian! += tanhscale\n        # inplace multiply!!!\n        temp ← zero(T)\n        temp += y![i] * expscale\n        SWAP(temp, y![i])\n        temp -= y![i] / expscale\n        temp → zero(T)\n        ~@routine\n    end\n\n    # affine the transform layer\n    i_affine!(y!, layer.W2, layer.b2, layer.y1a)\n    ~@routine\n    # clean up accumulated rounding error, since this memory is reused.\n    @safe layer.y1 .= zero(T)\n    @safe layer.sy1 .= zero(T)\nend","category":"page"},{"location":"examples/realnvp/#","page":"RealNVP network","title":"RealNVP network","text":"A realnvp network always transforms inputs reversibly. We update one half of x! a time, so that input and output memory space do not clash.","category":"page"},{"location":"examples/realnvp/#","page":"RealNVP network","title":"RealNVP network","text":"@i function realnvp!(x!::AbstractVector{T}, network::RealNVP{T}, logjacobian!) where T\n    @invcheckoff for i=1:length(network)\n        np ← length(x!)\n        if (i%2==0, ~)\n            @inbounds onelayer!(view(x!,np÷2+1:np), network[i], view(x!,1:np÷2), logjacobian!; islast=i==length(network))\n        else\n            @inbounds onelayer!(view(x!,1:np÷2), network[i], view(x!,np÷2+1:np), logjacobian!; islast=i==length(network))\n        end\n    end\nend","category":"page"},{"location":"examples/realnvp/#","page":"RealNVP network","title":"RealNVP network","text":"How to obtain the log-probability of a data.","category":"page"},{"location":"examples/realnvp/#","page":"RealNVP network","title":"RealNVP network","text":"@i function logp!(out!::T, x!::AbstractVector{T}, network::RealNVP{T}) where T\n    (~realnvp!)(x!, network, out!)\n    @invcheckoff for i = 1:length(x!)\n        @routine begin\n            xsq ← zero(T)\n            @inbounds xsq += x![i]^2\n        end\n        out! -= 0.5 * xsq\n        ~@routine\n    end\nend","category":"page"},{"location":"examples/realnvp/#","page":"RealNVP network","title":"RealNVP network","text":"The negative-log-likelihood loss function","category":"page"},{"location":"examples/realnvp/#","page":"RealNVP network","title":"RealNVP network","text":"@i function nll_loss!(out!::T, cum!::T, xs!::Matrix{T}, network::RealNVP{T}) where T\n    @invcheckoff for i=1:size(xs!, 2)\n        @inbounds logp!(cum!, view(xs!,:,i), network)\n    end\n    out! -= cum!/size(xs!, 2)\nend","category":"page"},{"location":"examples/realnvp/#Training-1","page":"RealNVP network","title":"Training","text":"","category":"section"},{"location":"examples/realnvp/#","page":"RealNVP network","title":"RealNVP network","text":"function train(x_data, model; num_epochs = 800)\n    num_vars = size(x_data, 1)\n    params = collect_params(model)\n    optimizer = Adam(; lr=0.01)\n    for epoch = 1:num_epochs\n        loss, a, b, c = nll_loss!(0.0, 0.0, copy(x_data), model)\n        if epoch % 50 == 1\n            println(\"epoch = $epoch, loss = $loss\")\n            display(showmodel(x_data, model))\n        end\n        _, _, _, gmodel = (~nll_loss!)(GVar(loss, 1.0), GVar(a), GVar(b), GVar(c))\n        g = grad.(collect_params(gmodel))\n        update!(params, grad.(collect_params(gmodel)), optimizer)\n        dispatch_params!(model, params)\n    end\n    return model\nend\n\nfunction showmodel(x_data, model; nsamples=2000)\n    scatter(x_data[1,1:nsamples], x_data[2,1:nsamples]; xlims=(-5,5), ylims=(-5,5))\n    zs = randn(2, nsamples)\n    for i=1:nsamples\n        realnvp!(view(zs, :, i), model, 0.0)\n    end\n    scatter!(zs[1,:], zs[2,:])\nend","category":"page"},{"location":"examples/realnvp/#","page":"RealNVP network","title":"RealNVP network","text":"you can find the training data in examples/ folder","category":"page"},{"location":"examples/realnvp/#","page":"RealNVP network","title":"RealNVP network","text":"x_data = Matrix(readdlm(joinpath(@__DIR__, \"train.dat\"))')\n\nimport Random; Random.seed!(22)\nmodel = random_realnvp(Float64, size(x_data, 1), 10, 10, 4; scale=0.1)","category":"page"},{"location":"examples/realnvp/#","page":"RealNVP network","title":"RealNVP network","text":"Before training, the distribution looks like (Image: before)","category":"page"},{"location":"examples/realnvp/#","page":"RealNVP network","title":"RealNVP network","text":"model = train(x_data, model; num_epochs=800)","category":"page"},{"location":"examples/realnvp/#","page":"RealNVP network","title":"RealNVP network","text":"After training, the distribution looks like (Image: before)","category":"page"},{"location":"examples/realnvp/#","page":"RealNVP network","title":"RealNVP network","text":"","category":"page"},{"location":"examples/realnvp/#","page":"RealNVP network","title":"RealNVP network","text":"This page was generated using Literate.jl.","category":"page"}]
}
